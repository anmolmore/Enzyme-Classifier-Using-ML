---
title: "Text Summarization via TextRank"
output:
  html_document:
    df_print: paged
Author: Voleti
---

Class,

A big reason people turn to text-an is the hope that it will offer succinct summaries of large corpora. One could somehow trade-off the effort to read through large bodies of text against obtaining a usable summary of key sentences - sort of an abstract.

Well, we'll use one way to do this - the textRank algo in R. A number of Py options are available too.

```{r}
## setup
suppressPackageStartupMessages({
  
if (!require(textrank)){install.packages("textrank")}

library(tidyverse)
library(tidytext)
library(textrank)
library(rvest)  # to scrape a document for summarization
  
})

```

## Scraping an Input Document

I'll use `rvest` to scrape an input document and summarize it. 

Reason I'm scraping a document and not a corpus (collection of documents) is that a single doc is likely to have a coherent structure, and hence a succinct abstract that can be obtained as a collection of key sentences. 

OTOH, a corpus could be a bunch of different themes flying in all directions and which maynot be easily amenable to a single summary (hence, a topic model is preferable for such a use-case).

Let's scrape a recent news-article about the famous fitness band FitBit's new product launch. See below.

```{r}
## read data via rvest scraping
url <- "http://time.com/5196761/fitbit-ace-kids-fitness-tracker/"
article <- read_html(url) %>%
  html_nodes('div[class="padded"]') %>%
  html_text()

# read the article
cat(article)
```

Read the article. What do you think? Simple, right? At this stage some key Qs arise. 

+ If you had to pickup a few (say 3 or 4) key sentences with which to make a summary or abstract, which ones would you choose?
+ In which order? 
+ Would the choice of any other sentences hamper the *quality* of the summary? 
+ Etc.

### Basic Text-an on the article

Let's perform some basic text-an via `tidytext` on this and then run our summarization algo.

```{r}
## load data into tibble

article_sentences <- tibble(text = article) %>%

  # sentence-tokenizing the article   
  unnest_tokens(sentence, text, token = "sentences") %>%
  
  # insert setence_id
  mutate(sentence_id = row_number()) %>%
  
  # drop frivolous stuff
  select(sentence_id, sentence)

head(article_sentences)  # view a few
```

More basic text-an. One could write one func to do all this and simply invoke. But I'll leave that to you to do.

```{r}

# word-tokenize too. for IDing keywords
article_words <- article_sentences %>%
  
    unnest_tokens(word, sentence) %>%
  
    # drop stopwords
    anti_join(stop_words, by = "word")

    head(article_words)

```

## Running Textrank

We run textrank via the `textrank_sentences()` func which requires 2 inputs, namely, 
[1] a DF of sentences, and [2] a DF of words in those sentences.

Do `?textrank_sentences` to see details on this function.

```{r}
system.time({

  article_summary <- textrank_sentences(data = article_sentences, 
                                      terminology = article_words)
})  # 0.65 secs

article_summary  # print the actual summary
```

Well, what do you think? Are the key sentences the machine picked out for us any good? 

How about we re-org the sentences for a better view? The plan is to explore the object *sentences* inside the article_summary object.

```{r}
# first examine the structure of the object of interest
str(article_summary[["sentences"]])
```

Now, let's extract what we need via dplyr's `pull()` func and re-org it for better display.

Below are the top 5 sentences in importance terms for the summary.

```{r}
# dplyr::pull() out the top 3 sents by textrank score (== eigenVal score?)
article_summary[["sentences"]] %>%
  arrange(desc(textrank)) %>% 
  slice(1:3) %>%  # dplyr::slice() chooses rows by their ordinal position in the tbl
  pull(sentence)

```

And the bottom 5 or least important sentences:

```{r}
# pull() out the bottom 3 sents by textrank score (== eigenVal score?)
article_summary[["sentences"]] %>%
  arrange(textrank) %>% 
  slice(1:3) %>%
  pull(sentence)
```

### Visualizing distribution of sentence importance

The idea is to plot textrank score vs sentence chronology 2 see where in passage highest score wale sentences occur.

```{r}
article_summary[["sentences"]] %>%
  
  ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
  
  geom_col() +
  theme_minimal() +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  
  labs(x = "Sentence",
       y = "TextRank score",
       title = "4 Most informative sentences appear within first half of sentences",
       subtitle = 'In article "Fitbits Newest Fitness Tracker Is Just for Kids"',
       caption = "Source: http://time.com/5196761/fitbit-ace-kids-fitness-tracker/")  # cool!
```

So, what does the distribution of the importance of sentences say about the article? Are the most important sentences uniformly spread throught the article? clustered at the beginning? at the end? Etc.

### How does textRank work?

Recall the COGs (co-occurrence graphs) we did in basic text-an? It displayd which tokens tended to co-occur in documents most frequently across the corpus. 

Textrank uses some network metrics from something like a cog but with sentences instead of documents. The most important or frequently occurring tokens in the article become central nodes - keywords - and the sentences with thehighest proportion of central nodes (eigenvector centrality scores) are chosen as the key sentences (in descending order of their centrality scores). 

You'll have a later course on network-an and these concepts will be introduced and explained in much greater detail then.


## One More Example

Above was kinda  a contrived example. Came from the package vignette and hence was designed to be good. 

Don't expect such neat and informative analyses and summaries to pop out everytime you run this. See an example from a desi news article below.

https://economictimes.indiatimes.com/news/economy/indicators/indias-economy-seen-limping-behind-china-as-modi-begins-second-term/articleshow/69580466.cms

I'll direct;ly input the text for this as ET news article links are notoriously prone to change.

```{r}
text <- "India probably lost its spot as the fastest growing major economy to China in the January-March quarter as a chill in domestic and global consumer demand hit manufacturers and service providers.

The slowing economy didn't stop voters giving Prime Minister Narendra Modi a landslide victory in an election concluded earlier this month.

But it puts an onus on him to deliver reforms that can truly unlock growth, which had waxed and waned during his first five years in office. A Reuters survey of economists forecast growth slipped to 6.3% annually in the three months ending in March, its slowest pace in six quarters.

If they are right, India would lag China, which notched 6.4 pct growth in the March quarter, for the first time in one-and-a-half years.

Modi is expected to begin his second term by prioritising growth in an economy that isn't creating enough new jobs for the millions of young Indians entering the labour market each month. 

His first task could be finding a new finance minister, as Arun Jaitley has asked to step aside due to health reasons. Whoever takes Jaitley's place will have to draw up a budget due to be presented in July.

The government is widely expected to deliver some fiscal stimulus while keeping the deficit at manageable levels. On the plus side, the Reserve Bank of India could have leeway to reduce interest rates as inflation remains subdued.

The gross domestic product data for January-March quarter and provisional estimates for the whole 2018/19 fiscal year ending in March will be released on Friday around 1200 GMT.

The RBI has lowered its economic growth forecast for 2019/20 fiscal year beginning April to 7.2%.

The central bank's monetary policy committee (MPC), which has cut policy rates by 50 basis points this year, is expected to cut the repo rate by a further 25 basis points at its June 4-6 meeting, bringing it to 5.75%, the lowest since July 2010.

Retail inflation has stayed below 3 percent for last six months, possibly low enough to take the risk of cutting rates without waiting to seeing whether the monsoon rainy season starting next month holds any danger of a spike in food prices.

Several indicators - automobile sales, rail freight, petroleum product consumption, domestic air traffic and imports indicate a slowdown in domestic consumption.

Corporate earnings hit a six-quarter low growth of 10.7% during January-March quarter on weakening consumer sentiment and softening commodity prices, ICRA, the Indian arm of the ratings agency Moody's said on Tuesday, citing a sample of over 300 companies.

The signs of slowdown in domestic demand are visible both in urban and rural areas, Federation of Indian Chambers of Commerce and Industry said in a statement earlier this week, while submitting pre-budget demands to the finance ministry.

Industry chambers have lobbied for a fiscal stimulus including a cut in corporate tax rates and lower interest rates."

```
Give the article a read pls. Note which sentences seem morst important and should form part of an article summary. In what order should these key sents come? Etc.

Below, I define a user defined func `text_summrzn()` to read in the text and output the stuff we saw above in one go.

Follow this func's flow carefully and note the inputs and outputs.

See below.

```{r}
article = text

## define func to do all the above in one shot
text_summrzn <- function(article, num_sentences=5){
  
  require(dplyr)
  require(magrittr)
  require(tidytext)
  require(textrank)
  
  ## load data into tibble
  article_sentences <- tibble(text = article) %>%
    unnest_tokens(sentence, text, token = "sentences") %>%    # sentence-tokenizing the article   
    mutate(sentence_id = row_number()) %>%    # insert setence_id
    select(sentence_id, sentence)  # drop frivolous stuff
  
  if (num_sentences > max(article_sentences$sentence_id)) {cat("More sentences in summary than in article."); break}
  
  ## word-tokenize too. for IDing keywords
  article_words <- article_sentences %>%
    unnest_tokens(word, sentence) %>%
    # drop stopwords
    anti_join(stop_words, by = "word")
  
  ## print summary
  article_summary <- textrank_sentences(data = article_sentences, 
                                        terminology = article_words)
  
  # top k sentences ka list
  output1 <- article_summary[["sentences"]] %>%
    arrange(desc(textrank)) %>% 
    slice(1:num_sentences) %>%  # dplyr::slice() chooses rows by their ordinal position in the tbl
    pull(sentence)
  
  # top k sentences ka plot
  require(ggplot2)
  output2 <- article_summary[["sentences"]] %>% 
    ggplot(aes(textrank_id, textrank, fill = textrank_id)) +
    
    geom_col() +
    theme_minimal() +
    scale_fill_viridis_c() +
    guides(fill = "none") +
    
    labs(x = "Sentence",
         y = "TextRank score",
         title = "Where do the most informative sentences appear in the article")
  
  return(list(output1, output2))
  
}  # func ends

```

Time to make rubber meet road. Let's execute the func and see.

```{r}
system.time({
  outp_list = text_summrzn(text, 5)    })  # 0.52 secs

outp_list[[1]]
outp_list[[2]]

```

Well, what do you think of the results?

Any sentences you would've added/dropped?

Sudhir

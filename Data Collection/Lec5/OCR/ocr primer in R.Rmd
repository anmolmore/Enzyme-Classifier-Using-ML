---
title: "An OCR Primer"
output:
  html_document:
    df_print: paged
---

## OCR in R

We've seen how to read text predefined in the `character` class via web-scraping (from html, xml, flat files etc).  

However, there may be a wealth of text data stored in images as well. Recovering these is the domain of optical character recognition (OCR).   

In what follows, we shallow-dive into a primer on OCR in R (and a little bit of py) using the tesseract (pytesseract) package, which provides R bindings to Google's open-source OCR engine by the same name.  

### Step 1: Preliminaries and Lib-loading

First, we set the working directory (wd) so that we can use relative paths going forward. Then load the sole required library `tesseract` which as we'll later see has a sole invoke-able func - `ocr()`.

```{r loading libraries}
 # prelims - set working directory ---------
   setwd("/Users/anmol/Dropbox/isb/Term1/Data Collection/Lec5/OCR")
   getwd()    # check if wd path is OK.

suppressPackageStartupMessages({   
  
 # load libraries including tesseract for OCR
  if (!(require(tesseract))) {install.packages("tesseract")};  library(tesseract)
  if (!(require(Hmisc))) {install.packages("Hmisc")};  library(Hmisc)
  if (!(require(xml2))) {install.packages("xml2")};  library(xml2)
  if (!(require(stringr))) {install.packages("stringr")};  library(stringr)
  if (!(require(tibble))) {install.packages("tibble")};  library(tibble)
  if (!(require(pdftools))) {install.packages("pdftools")};  library(pdftools)
  
  })

 ## load image data needed in png or jpg etc. 
   test.text = "./test-text.png"   
   
```
### Step 2: Run quick OCR and eval results

Do `?ocr` to see the arguments for this apparently simple looking func. 

In terms of workflow, the `ocr()` func takes an input an image URL, binarizes the image, passes it through tesseract and finally yields the output as an R object.

```{r running ocr}

 # Text output via standard ocr() func

 system.time({ 
	text <- ocr(test.text)   # only URL of image needed, not loaded image itself.
  })   # 0.28 secs

 cat(text)   # view result
```
How does the result compare with the original? Note that the font was the same throughout and the text was small. Imagine if the fonts were of varying types and sizes and colors ...  

Open and examine the image 'varying-text.jpg'. Analyze as above.

```{r varying-text-ocr}

 # OCR-analyzing varying text -------------
 system.time({
	varying.text = ocr("./varying-text.png")
	})    # 0.10 secs

  varying.text

  # Post-process OCR into multi-line text ---------
  # We'll use Hmisc::string.break.line()
   varying.text1 = string.break.line(varying.text)
   varying.text1
```
Well, what does an OCR-analysis of variously-format text (by font, size, background-contrast etc) give us then?

A lot of training is needed before the OCR engine can reliably pickup arbitrary formats.    

Options to train the OCR are also available (but out of scope for this primer). However, interested folks can always refer to:   

+  [Google tesseract engine options](http://www.sk-spell.sk.cx/tesseract-ocr-parameters-in-302-version)

+  [Tesseract Training Data](https://github.com/tesseract-ocr/tessdata)

and so on. For our purposes here, we stick to 'standard-formatted' text here on.

### Step 3: OCR-ring Tabular Information

Consider nutrition labels on F&B items mandated by law. These often come in tabular form. Open and examine 'the-nutritional-label.jpg' for OCR-an.

```{r nutrition-label-an}

  nutrition.text = "./the-nutritional-label.jpg" 
  system.time({ nutrition.text = ocr("./the-nutritional-label.jpg") })    # 5.4 secs
   
  nutrition.text   # view the text. check with original
  class(nutrition.text)
   
   # post-processing into multi-line text with Hmisc::string.break.line()
   library(Hmisc)   
   nutrition.text1 = string.break.line(nutrition.text)
   nutrition.text1
```
And what do we see above? Further processing into a tabular form is desirable but I'll leave it as a DIY exercise.  

### Step 4: Annotated hOCR formats

*hOCR* is an open standard of data representation for formatted text obtained from OCR. This [wikipedia link gives more information](https://en.wikipedia.org/wiki/HOCR).  

The hOCR definition encodes text, style, layout information, recognition confidence metrics and other information using XML.  

I'll use the file `test-text.png` to demo hOCR, thus:

```{r hOCR demo}

system.time({ 
    xml = ocr("./test-text.png", HOCR = TRUE) })   # 0.28 secs
 cat(xml)   # a markup file will display

```
The XML output contains a lot of hOCR metadata about the image file, and its main elements include (from the Wiki page):

+  different layout elements such as "ocr_par", "ocr_line", "ocrx_word"  
+  geometric information for each element with a bounding box "bbox"  
+  language information "lang"  
+  some confidence values "x_wconf"  

This XML output we can 'prettify' and serve into a dataframe, thus:

```{r xml 2 df}

## extracting neat df out of hocr metadata markup above
 library(xml2)
 library(stringr)
 library(tibble)

 doc <- read_xml(xml)   # note similarity with rvest::read_html()
 
 # Examine 'doc' and ID nodes & attributes of interest to mine.
 nodes <- xml_find_all(doc, ".//span[@class='ocrx_word']")    # using xpath instead of css selectors
   words <- xml_text(nodes)    # analogous to rvest::html_text()
   metatext <- xml_attr(nodes, 'title')  # ~ rvest::hml_attr()
 
 # extract geom co-ords of 'bounding box' or bbox around ocr text
 bbox <- str_replace(str_extract(metatext, "bbox [\\d ]+"), "bbox ", "")
 
 # Recognition confidence metrics 
 conf <- as.numeric(str_replace(str_extract(metatext, "x_wconf.*"), "x_wconf ", ""))
 
 # build df now
 tibble(confidence = conf, word = words, bbox = bbox)
 
```
### Step 5: OCR-ring PDF files

Finally, heading to a *Full roundtrip test*: rendering PDF to image and OCR it back to text. An analysis that combines pdftools and ocr.   

This example comes from the OCR vignette itself. The aim is to take *the* authoritative pdf introduction to the R platform/language from CRAN, save it as an image and then render it back to text via ocr. See below.  

```{r pdftools and tesseract}

library(pdftools)

 system.time({
  curl::curl_download("https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf", "R-intro.pdf")
  orig <- pdftools::pdf_text("R-intro.pdf")[1]  
 })    # 10.41 secs

 # view original text we are extracting
 orig  

 # Render pdf to png image using pdf_convert()
 system.time({
   img_file <- pdftools::pdf_convert("R-intro.pdf", format = 'tiff', pages = 1, dpi = 400)
 })    # 0.21 secs

 # Now Extract text from png image
 system.time({ text <- ocr(img_file) })    # 1.85 secs
   unlink(img_file)    # Deleting done Files and Directories
   cat(text)
   
```
OK, that covers another important data collection approach young data scientists are likely to find useful in the intersection-heavy world out there.  

Can we do OCR in Py? You bet! I'll putup some material out there reg the same.

Ciao.

Sudhir Voleti @ ISB.

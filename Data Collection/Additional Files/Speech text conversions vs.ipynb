{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to Text conversions using different API's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction and Background\n",
    "\n",
    "*Speech recognition* primarily involves recognition and **transcription** of spoken language to text. It can be done real-time similar to the way done in Google voice keyboards, auto transcribers etc. \n",
    "\n",
    "In this markdown we would *transcribe* an audio recording to text using Python.\n",
    "\n",
    "Note: Transcription should not be confused with *translation*, which means representing the meaning of a source language text in a target language. Also not to be confused with *transliteration* which  means representing accurate text spelling from one script to another.\n",
    "\n",
    "Reg the relevance of speech2text methods to DC, what business situations or use-cases might be out there?\n",
    "\n",
    "The example audio we'll use in our demonstration today is a small part of Donald Trump's speech during his presidential campaign. We'll use 'SpeechRecognition' package of python, which uses different API's based on the function invoked. \n",
    "\n",
    "Speech recognition engine/API support (in the SpeechRecognition package):\n",
    "\n",
    "<ul>\n",
    "    <li>CMU Sphinx (works offline)</li>\n",
    "    <li>Google Speech Recognition</li>\n",
    "    <li>Google Cloud Speech API</li>\n",
    "    <li>Wit.ai</li>\n",
    "    <li>Microsoft Bing Voice Recognition</li>\n",
    "    <li>Houndify API</li>\n",
    "    <li>IBM Speech to Text</li>\n",
    "    <li>Snowboy Hotword Detection (works offline)</li>\n",
    "</ul>\n",
    "\n",
    "Based on different situations, we may choose an offline engine or an online API. Each has its pros and cons. Any guesses as to what these might be? Which one would you prefer a priori?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Am using the *import sys* and *!pip install moduleName* method to check for already present packages and if not present to download and pip-install them. \n",
    "\n",
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 32.8MB 993kB/s ta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.8.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time  # for timing funcs\n",
    "import speech_recognition as sr\n",
    "sr.__version__  # which version is installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/audio py files/data/trump_speech.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3031ce2436b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecognizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# creating a Recognizer instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_FILE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# read the entire audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# attempt to read the file as WAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename_or_fileobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlittle_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# RIFF WAV is a little-endian format (most ``audioop`` operations assume that the frames are stored in little-endian form)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/wave.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWave_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mWave_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/wave.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_i_opened_the_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_i_opened_the_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# else, assume it is an open file object already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/audio py files/data/trump_speech.wav'"
     ]
    }
   ],
   "source": [
    "# obtain path to \"trump_speech.wav\" in the same folder as this script\n",
    "from os import path\n",
    "path1 = 'D:/audio py files/'\n",
    "AUDIO_FILE = path1 + \"data/trump_speech.wav\"\n",
    "\n",
    "# use the audio file as the audio source\n",
    "t1 = time.time()\n",
    "\n",
    "r = sr.Recognizer()  # creating a Recognizer instance\n",
    "\n",
    "with sr.AudioFile(AUDIO_FILE) as source:\n",
    "    audio = r.record(source)  # read the entire audio file\n",
    "\n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speech_recognition.AudioData"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sphinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pocketsphinx in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.1.15)\n"
     ]
    }
   ],
   "source": [
    "# check for installation\n",
    "import sys\n",
    "!pip install pocketsphinx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recognizer class is crucial in SpeechRecognition. Below, look for the *r.recognize_sphinx()* func.\n",
    "\n",
    "Each *recognize_xyz()* method will throw a *speech_recognition.RequestError* exception if the API is unreachable. \n",
    "\n",
    "For recognize_sphinx(), this could happen as the result of a missing, corrupt or incompatible Sphinx installation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text from audio is:\n",
      "my administration has accomplished more than almost any administration in the history of our country martyrs sojourn to type their reaction of that so united states will not tell you how to live for work or worship we only ask that you are our sovereignty in return i would like to thank chairman kim for his carriage and four of the steps he is taking so much more remains to be that our shared goals must be the de escalation of military conflict along with a political solution that honors the will of the syrian people the united states will respond if chemical weapons are employed by the us side\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.768925428390503"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recognize speech using Sphinx\n",
    "t1 = time.time()\n",
    "\n",
    "try:\n",
    "    print(\"The text from audio is:\\n\" + r.recognize_sphinx(audio))\n",
    "except:\n",
    "    print(\"There was error processing audio\")\n",
    "    \n",
    "time.time() - t1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above took 20 odd secs for a 1 minute clip. Significant time taken. \n",
    "\n",
    "So, for large audio files, one way maybe to break into pieces and parallelize the transcription op. This is easy with offline functions.\n",
    "\n",
    "#### Assessing Transcription quality\n",
    "\n",
    "You've heard the clip. How clear was it? How hard or easy was it to follow? \n",
    "\n",
    "You've read the transcription above. Compare the two - speech and text. Arrive at some estimate of 'transcription quality'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google speech recognition\n",
    "\n",
    "Caution: The default API key for Goog speech services provided by SpeechRecognition is for *testing* purposes only. And Google may revoke it at any time. \n",
    "\n",
    "It is **not** a good idea to use the Google Web Speech API in production. \n",
    "\n",
    "Even with a valid API key, you’ll be limited to only 50 requests per day, and apparently there's no way to raise this quota. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text from audio is:\n",
      "my administration is more than most any administration in the history of America\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.358235597610474"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "# recognize speech using Google Speech Recognition (default without credentials)   \n",
    "try:\n",
    "    print(\"The text from audio is:\\n\" + r.recognize_google(audio))\n",
    "except:\n",
    "    print(\"There was error processing audio\")\n",
    "    \n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output was all I could get. Insufficient to judge transcription quality on its basis.\n",
    "\n",
    "All said and done, between sphinx and goog, I'd go with the former, for now.\n",
    "\n",
    "Next, let's look at another service on offer from __[SoundHound](https://www.houndify.com/)__.\n",
    "\n",
    "P.S. You might wanna check out the website for the cool AI-ish promises being made aajkal..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Houndify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text from audio is:\n",
      "my administration has accomplished more than almost any administration in the history of our country america's center didn't expect that reaction but that's ok united states will not tell you how to live for work or worship we only ask that you honor our sovereignty in return i would like to thank chairman kim for his courage and for the steps he has taken though much work remains to be done our shared goals must be the de escalation of military conflict along with a political solution that honors the will of the syrian people the united states will respond if chemical weapons are deployed by the a-side\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28.29644799232483"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recognize speech using Houndify\n",
    "HOUNDIFY_CLIENT_ID = \"K-PDcIt1-UIYGmwLbe19mg==\"  \n",
    "HOUNDIFY_CLIENT_KEY = \"glsGO73hcZPBgdQ8toWXkGSsqS-lqcQyPTyxd71CsF3byCrG-0uRhx4D5cY4ulTOfSCVXGMSyVvordcxiBLFJA==\"\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "try:\n",
    "    print(\"The text from audio is:\\n\" + r.recognize_houndify(audio, client_id=HOUNDIFY_CLIENT_ID, client_key=HOUNDIFY_CLIENT_KEY))\n",
    "except:\n",
    "    print(\"There was error processing audio\")\n",
    "    \n",
    "time.time() - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took longer (at ~ 28 secs) but a quick assessment shows its way better quality. Its API based rather than offline. Tradeoffs galore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other functions which are a part of this package are:\n",
    "\n",
    "<ul>\n",
    "    <li>recognize_bing()</li>\n",
    "    <li>recognize_google_cloud()</li>\n",
    "    <li>recognize_ibm(): IBM Speech to Text</li>\n",
    "    <li>recognize_wit(): Wit.ai</li>\n",
    "</ul>\n",
    "\n",
    "A note worthy mention (outside of SpeechRecognition) bt within the py ecosystem is __[APIAI](https://pypi.org/project/apiai/)__. \n",
    "\n",
    "Among the above, wit and apiai—offer built-in features, like natural language processing for identifying a speaker’s intent, which go beyond basic speech recognition. \n",
    "\n",
    "Others, like sphinx and google-cloud-speech, focus solely on speech-to-text conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data collection\n",
    "\n",
    "So far I've assumed you've ready-made .wav (or other format) audio files available for analysis. Else, one would have to collect speech data. \n",
    "\n",
    "This however will require a few dependencies. Notably, the **PyAudio** package is needed for capturing microphone input.\n",
    "\n",
    "Nowadays recording conversations is easy via the mobile phone, so am not stressing this aspect much. Desktops attached with a microphone can also directly take voice recordings and convert to .wav files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Speech conversions using different API's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to Speech achieves the opposite objective of Speech to text. \n",
    "\n",
    "The practical applications involves audiobooks, voice guides in tourist attractions and etc. We would use 'gTTS' package for text-to-speech conversion. \n",
    "\n",
    "gTTS stands for 'google Text To Speech' and uses google voice API for coversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gTTS\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/37/f55346a736278f0eb0ae9f7edee1a61028735ef0010db68a2e6fcd0ece56/gTTS-2.0.3.tar.gz\n",
      "Requirement already satisfied: six in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gTTS) (1.11.0)\n",
      "Collecting bs4 (from gTTS)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: click in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gTTS) (6.7)\n",
      "Requirement already satisfied: requests in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gTTS) (2.18.4)\n",
      "Collecting gtts_token (from gTTS)\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/25/ca6e9cd3275bfc3097fe6b06cc31db6d3dfaf32e032e0f73fead9c9a03ce/gTTS-token-1.1.3.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from bs4->gTTS) (4.6.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->gTTS) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->gTTS) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->gTTS) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\20052\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->gTTS) (2018.4.16)\n",
      "Building wheels for collected packages: gTTS, bs4, gtts-token\n",
      "  Building wheel for gTTS (setup.py): started\n",
      "  Building wheel for gTTS (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\20052\\AppData\\Local\\pip\\Cache\\wheels\\ac\\d3\\52\\db6c154b20dfaab7e0b514eb5eef92cecd057e40e16fdda58b\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\20052\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "  Building wheel for gtts-token (setup.py): started\n",
      "  Building wheel for gtts-token (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\20052\\AppData\\Local\\pip\\Cache\\wheels\\dd\\11\\61\\33f7e51bf545e910552b2255eead2a7cd8ef54064b46dceb34\n",
      "Successfully built gTTS bs4 gtts-token\n",
      "Installing collected packages: bs4, gtts-token, gTTS\n",
      "Successfully installed bs4-0.0.1 gTTS-2.0.3 gtts-token-1.1.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!pip install gTTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7452943325042725"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS \n",
    "import os \n",
    "  \n",
    "mytext = 'Welcome to Indian School of Business!'\n",
    "\n",
    "t1 = time.time()\n",
    "# Language in which you want to convert \n",
    "language = 'en'\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)  \n",
    "\n",
    "time.time() - t1   # runtime in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the audio saved with below filename in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myobj.save(path1 + \"welcome.mp3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'af': 'Afrikaans',\n",
       " 'sq': 'Albanian',\n",
       " 'ar': 'Arabic',\n",
       " 'hy': 'Armenian',\n",
       " 'bn': 'Bengali',\n",
       " 'bs': 'Bosnian',\n",
       " 'ca': 'Catalan',\n",
       " 'hr': 'Croatian',\n",
       " 'cs': 'Czech',\n",
       " 'da': 'Danish',\n",
       " 'nl': 'Dutch',\n",
       " 'en': 'English',\n",
       " 'eo': 'Esperanto',\n",
       " 'et': 'Estonian',\n",
       " 'tl': 'Filipino',\n",
       " 'fi': 'Finnish',\n",
       " 'fr': 'French',\n",
       " 'de': 'German',\n",
       " 'el': 'Greek',\n",
       " 'hi': 'Hindi',\n",
       " 'hu': 'Hungarian',\n",
       " 'is': 'Icelandic',\n",
       " 'id': 'Indonesian',\n",
       " 'it': 'Italian',\n",
       " 'ja': 'Japanese',\n",
       " 'jw': 'Javanese',\n",
       " 'km': 'Khmer',\n",
       " 'ko': 'Korean',\n",
       " 'la': 'Latin',\n",
       " 'lv': 'Latvian',\n",
       " 'mk': 'Macedonian',\n",
       " 'ml': 'Malayalam',\n",
       " 'mr': 'Marathi',\n",
       " 'my': 'Myanmar (Burmese)',\n",
       " 'ne': 'Nepali',\n",
       " 'no': 'Norwegian',\n",
       " 'pl': 'Polish',\n",
       " 'pt': 'Portuguese',\n",
       " 'ro': 'Romanian',\n",
       " 'ru': 'Russian',\n",
       " 'sr': 'Serbian',\n",
       " 'si': 'Sinhala',\n",
       " 'sk': 'Slovak',\n",
       " 'es': 'Spanish',\n",
       " 'su': 'Sundanese',\n",
       " 'sw': 'Swahili',\n",
       " 'sv': 'Swedish',\n",
       " 'ta': 'Tamil',\n",
       " 'te': 'Telugu',\n",
       " 'th': 'Thai',\n",
       " 'tr': 'Turkish',\n",
       " 'uk': 'Ukrainian',\n",
       " 'vi': 'Vietnamese',\n",
       " 'cy': 'Welsh',\n",
       " 'zh-cn': 'Chinese (Mandarin/China)',\n",
       " 'zh-tw': 'Chinese (Mandarin/Taiwan)',\n",
       " 'en-us': 'English (US)',\n",
       " 'en-ca': 'English (Canada)',\n",
       " 'en-uk': 'English (UK)',\n",
       " 'en-gb': 'English (UK)',\n",
       " 'en-au': 'English (Australia)',\n",
       " 'en-gh': 'English (Ghana)',\n",
       " 'en-in': 'English (India)',\n",
       " 'en-ie': 'English (Ireland)',\n",
       " 'en-nz': 'English (New Zealand)',\n",
       " 'en-ng': 'English (Nigeria)',\n",
       " 'en-ph': 'English (Philippines)',\n",
       " 'en-za': 'English (South Africa)',\n",
       " 'en-tz': 'English (Tanzania)',\n",
       " 'fr-ca': 'French (Canada)',\n",
       " 'fr-fr': 'French (France)',\n",
       " 'pt-br': 'Portuguese (Brazil)',\n",
       " 'pt-pt': 'Portuguese (Portugal)',\n",
       " 'es-es': 'Spanish (Spain)',\n",
       " 'es-us': 'Spanish (United States)'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get supported languages ka list\n",
    "import gtts.lang\n",
    "gtts.lang.tts_langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6135022640228271"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying something in Hindi\n",
    "mytext = 'hum terey bin ab reh nahi sakteyy, terey binaa kyaa wajood meraa'\n",
    "\n",
    "t1 = time.time()\n",
    "# Language in which you want to convert \n",
    "language = 'hi'\n",
    "myobj = gTTS(text=mytext, lang=language, slow=False)  \n",
    "\n",
    "# save as mp3 file\n",
    "myobj.save(path1 + \"bollywood.mp3\")\n",
    "\n",
    "time.time() - t1   # runtime in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the above file bollywood.mp3 and see for yourself. \n",
    "\n",
    "Now its an unreasonable standard, expecting the machine to 'sing' but did it at least talk prose in Hindi properly? \n",
    "\n",
    "Could we improve the pronunciation by changig input text? Etc and other things.\n",
    "\n",
    "I've now half a mind to try some of my olden favorite and *slow* English classics (Think Bob Dylan's \"Blowing in the wind\" or Led Zep's \"tairway to Heaven\" to test TTS quality....\n",
    "\n",
    "But, shall logoff here for now on this notebook.\n",
    "\n",
    "Ciao\n",
    "Sudhir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

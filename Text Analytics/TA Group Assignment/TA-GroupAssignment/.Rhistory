windowsFonts(devanew=windowsFont("Devanagari new normal"))
# However vector support devnagri scripts
head(x$token)    # Yay!
table(x$xpos)  # std penn treebank based POStags
table(x$upos)  # UD based postags
# So what're the most common nouns? verbs?
all_nouns = x %>% subset(., upos %in% "NOUN"); all_nouns$token[1:20]
top_nouns = txt_freq(all_nouns$lemma)
head(top_nouns$key, 20)
all_verbs = x %>% subset(., upos %in% "VERB")
top_verbs = txt_freq(all_verbs$lemma)
head(top_verbs$key, 20)
library(wordcloud)
wordcloud(words = top_nouns$key,
freq = top_nouns$freq,
min.freq = 2,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(6, "Dark2"))
install.packages("Rserve")
library(Rserve) # install+load installr
library("Rserve")
install.packages("Rserve")
install.packages("Rserve")
r
# setup code block
suppressPackageStartupMessages({
if (!require(udpipe)){install.packages("udpipe")}
if (!require(textrank)){install.packages("textrank")}
if (!require(lattice)){install.packages("lattice")}
if (!require(igraph)){install.packages("igraph")}
if (!require(ggraph)){install.packages("ggraph")}
if (!require(wordcloud)){install.packages("wordcloud")}
library(udpipe)
library(textrank)
library(lattice)
library(igraph)
library(ggraph)
library(ggplot2)
library(wordcloud)
library(stringr)
})
# setup working dir
setwd('C:\\Users\\Dharani Kiran\\OneDrive\\ISB\\Term 1\\Text Analytics\\Session 4\\Session 4 Materials\\');  getwd()
# setup code block
suppressPackageStartupMessages({
if (!require(udpipe)){install.packages("udpipe")}
if (!require(textrank)){install.packages("textrank")}
if (!require(lattice)){install.packages("lattice")}
if (!require(igraph)){install.packages("igraph")}
if (!require(ggraph)){install.packages("ggraph")}
if (!require(wordcloud)){install.packages("wordcloud")}
library(udpipe)
library(textrank)
library(lattice)
library(igraph)
library(ggraph)
library(ggplot2)
library(wordcloud)
library(stringr)
})
# setup working dir
setwd('C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Session 4\\Session 4 Materials\\');  getwd()
# Download language models in English, Spanish and Hindi (say). Uncomment or copy from LMS.
#system.time({
#  ud_model_english <- udpipe_download_model(language = "english")  # 3.05 secs
#  ud_model_spanish <- udpipe_download_model(language = "spanish")  #  secs
#  ud_model_hindi <- udpipe_download_model(language = "hindi")  # secs
#	}) # ~ 114 secs, depends on comp & connection speeds.
# get data first
require(stringr)
nokia = readLines('https://raw.githubusercontent.com/sudhir-voleti/sample-data-sets/master/text%20analysis%20data/amazon%20nokia%20lumia%20reviews.txt')
nokia  =  str_replace_all(nokia, "<.*?>", "") # get rid of html junk
str(nokia)
# ?udpipe_download_model   # for langu listing and other details
# load english model for annotation from working dir
english_model = udpipe_load_model("./english-ewt-ud-2.4-190531.udpipe")  # file_model only needed
# now annotate text dataset using ud_model above
# system.time({   # ~ depends on corpus size
x <- udpipe_annotate(english_model, x = nokia) #%>% as.data.frame() %>% head()
x <- as.data.frame(x)
#	})  # 13.76 secs
# str(x);
head(x, 4)
source("https://raw.githubusercontent.com/sudhir-voleti/text-topic-analysis-shinyapp/master/dependency-text-topic-analysis-shinyapp.R")
source("https://raw.githubusercontent.com/sudhir-voleti/text-topic-analysis-shinyapp/master/dependency-text-topic-analysis-shinyapp.R")
runGitHub("basic-text-analysis-shinyapp", "sudhir-voleti")
runGitHub("text-topic-analysis-shinyapp", "sudhir-voleti")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("XML")
# setup chunk
suppressPackageStartupMessages({
library(rvest)   # for scraping funcs
library(XML)     # for parsing and viewing a nodeset
library(magrittr) # for piping ops
library(dplyr)   # fopr dataframe ops
})
install.packages("dplyr")
# setup chunk
suppressPackageStartupMessages({
library(rvest)   # for scraping funcs
library(XML)     # for parsing and viewing a nodeset
library(magrittr) # for piping ops
library(dplyr)   # fopr dataframe ops
})
knitr::opts_chunk$set(echo = TRUE)
install.packages("knitr")
install.packages("knitr")
knitr::opts_chunk$set(echo = TRUE)
if (!require(shiny)) {install.packages('shiny')}
library(tidyverse)
install.packages("tidyverse")
install.packages("tidytext")
library(tidyverse)
library(tidytext)
setwd("C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Session 3\\Session 3 Materials\\Session 3 Materials\\"); getwd()
# Consider 4 classics whose unlabeled chapters have been jumbled up.
titles <- c("Twenty Thousand Leagues under the Sea",
"The War of the Worlds",
"Pride and Prejudice",
"Great Expectations")
# using LTMs to reorganize the chapters
library(gutenbergr)
install.packages(c("gutenbergr", "tidyverse"))
install.packages(c("gutenbergr", "tidyverse"))
install.packages("gutenbergr")
library(tidyverse)
library(tidytext)
setwd("C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Session 3\\Session 3 Materials\\Session 3 Materials\\"); getwd()
# Consider 4 classics whose unlabeled chapters have been jumbled up.
titles <- c("Twenty Thousand Leagues under the Sea",
"The War of the Worlds",
"Pride and Prejudice",
"Great Expectations")
# using LTMs to reorganize the chapters
library(gutenbergr)
# system.time({
books <- gutenberg_works(title %in% titles) %>%
gutenberg_download(meta_fields = "title")
# })  # 5 secs
##  Alternately, read from disk directly as below
# saveRDS(books, "./books.Rds")
# books = readRDS("./books.Rds");  class(books)
books    # tibble: 51,663 x 3. cols = {gutenberg_id, text, title}
View(books)
View(books)
library(stringr)
# divide each book chapter into separate document.
by_chapter = books %>%
group_by(title) %>%
# detecting chapters using regex
mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
ungroup() %>%
filter(chapter > 0) %>%
unite(document, title, chapter)
# split into words
by_chapter_word = by_chapter %>%
unnest_tokens(word, text)
# find document-word counts
word_counts = by_chapter_word %>%
anti_join(stop_words) %>%
count(document, word, sort = TRUE) %>%
ungroup()
word_counts    # A tibble: 104,722 x 3. cols = {doc, word, n}
# now cast into dtm
chapters_dtm = word_counts %>%
cast_dtm(document, word, n)
chapters_dtm   # DocumentTermMatrix (documents: 193, terms: 18215)
# Basic Text Analysis App
source("https://raw.githubusercontent.com/sudhir-voleti/basic-text-analysis-shinyapp/master/dependency-basic-text-analysis-shinyapp.R")
runGitHub("basic-text-analysis-shinyapp", "sudhir-voleti")
try(require(shiny) || install.packages("shiny"))
try(require(text2vec) || install.packages("text2vec"))
try(require(tm) || install.packages("tm"))
try(require(tokenizers) || install.packages("tokenizers"))
try(require(wordcloud) || install.packages("wordcloud"))
try(require(slam) || install.packages("slam"))
try(require(stringi) || install.packages("stringi"))
try(require(magrittr) || install.packages("magrittr"))
try(require(tidytext) || install.packages("tidytext"))
try(require(dplyr) || install.packages("dplyr"))
try(require(tidyr) || install.packages("tidyr"))
try(require(igraph)|| install.packages("igraph"))
library(shiny)
library(text2vec)
library(tm)
library(tokenizers)
library(wordcloud)
library(slam)
library(stringi)
library(magrittr)
library(tidytext)
library(dplyr)
library(tidyr)
library(igraph)
try(require(shiny) || install.packages("shiny"))
try(require(text2vec) || install.packages("text2vec"))
try(require(tm) || install.packages("tm"))
try(require(tokenizers) || install.packages("tokenizers"))
try(require(wordcloud) || install.packages("wordcloud"))
try(require(slam) || install.packages("slam"))
try(require(stringi) || install.packages("stringi"))
try(require(magrittr) || install.packages("magrittr"))
try(require(tidytext) || install.packages("tidytext"))
try(require(dplyr) || install.packages("dplyr"))
try(require(tidyr) || install.packages("tidyr"))
try(require(igraph)|| install.packages("igraph"))
library(shiny)
library(text2vec)
library(tm)
library(tokenizers)
library(wordcloud)
library(slam)
library(stringi)
library(magrittr)
library(tidytext)
library(dplyr)
library(tidyr)
library(igraph)
try(require(shiny) || install.packages("shiny"))
try(require(text2vec) || install.packages("text2vec"))
try(require(tm) || install.packages("tm"))
try(require(tokenizers) || install.packages("tokenizers"))
try(require(wordcloud) || install.packages("wordcloud"))
try(require(slam) || install.packages("slam"))
try(require(stringi) || install.packages("stringi"))
try(require(magrittr) || install.packages("magrittr"))
try(require(tidytext) || install.packages("tidytext"))
try(require(dplyr) || install.packages("dplyr"))
try(require(tidyr) || install.packages("tidyr"))
try(require(igraph)|| install.packages("igraph"))
library(shiny)
library(text2vec)
library(tm)
library(tokenizers)
library(wordcloud)
library(slam)
library(stringi)
library(magrittr)
library(tidytext)
library(dplyr)
library(tidyr)
library(igraph)
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/TextAnalyticsGroupAssignment.R")
runGitHub("TextAnalyticsGroupAssignment", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.R")
runGitHub("ui", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.R")
runGitHub("ui", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.R")
runGitHub("ui", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.R")
runGitHub("ui", "dharanikiran")
runGitHub("TextAnalytics/ui.r", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.R")
runGitHub("TextAnalytics/ui.r", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.R")
runGitHub("TextAnalytics/ui.r", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.R")
runGitHub("TextAnalytics/ui.r", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/ui.r")
runGitHub("ui", "dharanikiran")
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/master/ui.r")
runGitHub("ui", "dharanikiran")
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/master/TextAnalyticsGroupAssignment.r")
runGitHub("TextAnalyticsGroupAssignment", "dharanikiran")
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
shiny::runApp('C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment')
# Basic Text Analysis App
source("https://raw.githubusercontent.com/dharanikiran/TextAnalytics/master/TextAnalyticsGroupAssignment.r")
runGitHub("TextAnalyticsGroupAssignment", "dharanikiran")
if (!require(udpipe)){install.packages("udpipe")}
if (!require(textrank)){install.packages("textrank")}
if (!require(lattice)){install.packages("lattice")}
if (!require(igraph)){install.packages("igraph")}
if (!require(ggraph)){install.packages("ggraph")}
if (!require(wordcloud)){install.packages("wordcloud")}
library(udpipe)
library(textrank)
library(lattice)
library(igraph)
library(ggraph)
library(ggplot2)
library(wordcloud)
library(stringr)
require(stringr)
InputDocument = readLines("C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Group Assignment\\Donald Trump.txt")
CleanDocument  =  str_replace_all(InputDocument, "<.*?>", "") # get rid of html junk
str(CleanDocument)
english_model = udpipe_load_model("./english-ewt-ud-2.4-190531.udpipe")
english_model = udpipe_load_model("C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Group Assignment\\english-ewt-ud-2.4-190531.udpipe")  # file_model only needed
x <- udpipe_annotate(english_model, x = CleanDocument)
# load english model for annotation from working dir
english_model = udpipe_load_model("C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Group Assignment\\english-ewt-ud-2.4-190531.udpipe")  # file_model only needed
# now annotate text dataset using ud_model above
x <- udpipe_annotate(english_model, x = CleanDocument) #%>% as.data.frame() %>% head()
x <- as.data.frame(x)
print(x)
head(x, 4)
all_nouns = x %>% subset(., upos %in% "NOUN")
top_nouns = txt_freq(all_nouns$lemma)  # txt_freq() calcs noun freqs in desc order
head(top_nouns, 10)
all_verbs = x %>% subset(., upos %in% "VERB")
top_verbs = txt_freq(all_verbs$lemma)
head(top_verbs, 10)
document_colloc <- keywords_collocation(x = x,   # try ?keywords_collocation
term = "token",
group = c("doc_id", "paragraph_id", "sentence_id"),
ngram_max = 4)  # 0.42 secs
str(document_colloc)
document_colloc %>% head()
document_cooc <- cooccurrence(   	# try `?cooccurrence` for parm options
x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma",
group = c("doc_id", "paragraph_id", "sentence_id"))  # 0.02 secs
# str(nokia_cooc)
head(document_cooc)
document_cooc_gen <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ")) # 0.00 secs
# str(nokia_cooc_gen)
head(document_cooc_gen)
# Skipgram based Co-occurrences: How frequent do words follow one another within skipgram number of words
document_cooc_skipgm <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"),
skipgram = 4)  # 0.05 secs
# str(nokia_cooc_skipgm)
head(document_cooc_skipgm)  # sorted in descending order
wordnetwork <- head(document_cooc, 50)
wordnetwork <- igraph::graph_from_data_frame(wordnetwork) # needs edgelist in first 2 colms.
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "orange") +
geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
theme_graph(base_family = "Arial Narrow") +
theme(legend.position = "none") +
labs(title = "Cooccurrences within 3 words distance", subtitle = "Nouns & Adjective")
table(x$xpos)  # std penn treebank based POStags
table(x$upos)  # UD based postags
# So what're the most common nouns? verbs?
all_nouns = x %>% subset(., upos %in% "NOUN"); all_nouns$token[1:20]
top_nouns = txt_freq(all_nouns$lemma)
head(top_nouns$key, 20)
all_verbs = x %>% subset(., upos %in% "VERB")
top_verbs = txt_freq(all_verbs$lemma)
head(top_verbs$key, 20)
library(wordcloud)
wordcloud(words = top_nouns$key,
freq = top_nouns$freq,
min.freq = 2,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(6, "Dark2"))
require(stringr)
InputDocument = readLines("C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Group Assignment\\Donald Trump.txt")
CleanDocument  =  str_replace_all(InputDocument, "<.*?>", "")
str(CleanDocument)
# load english model for annotation from working dir
english_model = udpipe_load_model("C:\\Users\\Dharani Kiran\\OneDrive - Indian School of Business\\ISB\\Term 1\\Text Analytics\\Group Assignment\\english-ewt-ud-2.4-190531.udpipe")  # file_model only needed
# now annotate text dataset using ud_model above
x <- udpipe_annotate(english_model, x = CleanDocument)
Annotated_Data <- as.data.frame(x)
print(Annotated_Data)
#	})  # 13.76 secs
head(Annotated_Data, 4)
table(Annotated_Data$xpos)  # std penn treebank based POStags
table(Annotated_Data$upos)  # UD based postags
# So what're the most common nouns? verbs?
all_nouns = Annotated_Data %>% subset(., upos %in% "NOUN")
top_nouns = txt_freq(all_nouns$lemma)  # txt_freq() calcs noun freqs in desc order
head(top_nouns, 10)
all_verbs = Annotated_Data %>% subset(., upos %in% "VERB")
top_verbs = txt_freq(all_verbs$lemma)
head(top_verbs, 10)
document_colloc <- keywords_collocation(Annotated_Data = Annotated_Data,   # try ?keywords_collocation
term = "token",
group = c("doc_id", "paragraph_id", "sentence_id"),
ngram_max = 4)  # 0.42 secs
document_colloc <- keywords_collocation(x = Annotated_Data,   # try ?keywords_collocation
term = "token",
group = c("doc_id", "paragraph_id", "sentence_id"),
ngram_max = 4)  # 0.42 secs
str(document_colloc)
document_colloc %>% head()
document_cooc <- cooccurrence(   	# try `?cooccurrence` for parm options
x = subset(Annotated_Data, upos %in% c("NOUN", "ADJ")),
term = "lemma",
group = c("doc_id", "paragraph_id", "sentence_id"))  # 0.02 secs
# str(nokia_cooc)
head(document_cooc)
# general (non-sentence based) Co-occurrences
document_cooc_gen <- cooccurrence(x = Annotated_Data$lemma,
relevant = x$upos %in% c("NOUN", "ADJ")) # 0.00 secs
# str(nokia_cooc_gen)
head(document_cooc_gen)
document_cooc_skipgm <- cooccurrence(x = x$lemma,
relevant = x$upos %in% c("NOUN", "ADJ"),
skipgram = 4)  # 0.05 secs
head(document_cooc_skipgm)
wordnetwork <- head(document_cooc, 30)
wordnetwork <- igraph::graph_from_data_frame(wordnetwork) # needs edgelist in first 2 colms.
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "orange") +
geom_node_text(aes(label = name), col = "darkgreen", size = 4) +
theme_graph(base_family = "Arial Narrow") +
theme(legend.position = "none") +
labs(title = "Cooccurrences within 3 words distance", subtitle = "Nouns & Adjective")
wordcloud(words = top_nouns$key,
freq = top_nouns$freq,
min.freq = 2,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(6, "Dark2"))
View(Annotated_Data)
View(Annotated_Data)
runGitHUb("TA-GroupAssignment","dharanikiran")
if (!require(shiny)) {install.packages("shiny")}
if (!require(shinythemes)) {install.packages("shinythemes")}
if (!require(wordcloud)) {install.packages("wordcloud")}
if (!require(udpipe)) {install.packages("udpipe")}
if (!require(ggraph)) {install.packages("ggraph")}
if (!require(igraph)) {install.packages("igraph")}
if (!require(stringr)) {install.packages("stringr")}
library(shiny)
library(shinythemes)
library(wordcloud)
library(udpipe)
library(ggraph)
library(igraph)
library(stringr)
runGitHUb("TA-GroupAssignment","dharanikiran")
runGitHub("TA-GroupAssignment","dharanikiran")
load("C:/Users/Dharani Kiran/OneDrive - Indian School of Business/ISB/Term 1/Text Analytics/Group Assignment/Test/Test/.RData")
try(require("shiny")||install.packages("shiny"))
try(require("nFactors")||install.packages("nFactors"))
try(require("qgraph")||install.packages("qgraph"))
try(require("corrplot")||install.packages("corrplot"))
library("shiny")
library("nFactors")
library("qgraph")
library("corrplot")
if (!require(udpipe)){install.packages("udpipe")}
if (!require(textrank)){install.packages("textrank")}
if (!require(lattice)){install.packages("lattice")}
if (!require(igraph)){install.packages("igraph")}
if (!require(ggraph)){install.packages("ggraph")}
if (!require(wordcloud)){install.packages("wordcloud")}
if (!require(DT)){install.packages("DT")}
library(udpipe)
library(textrank)
library(lattice)
library(igraph)
library(ggraph)
library(ggplot2)
library(wordcloud)
library(stringr)
library(DT)
runGitHub("TA-GroupAssignment","dharanikiran")
runGitHub("TA-GroupAssignment", username = getOption("github.user"), ref = "master",
subdir = NULL, port = NULL,
launch.browser = getOption("shiny.launch.browser", interactive())
runGitHub("TA-GroupAssignment", username = getOption("github.user"), ref = "master",
subdir = NULL, port = NULL,
launch.browser = getOption("shiny.launch.browser", interactive()))
runGitHub("TA-GroupAssignment", username = getOption("github.user"), ref = "master",
subdir = NULL, port = NULL,
launch.browser = getOption("shiny.launch.browser", interactive()))
runGitHub("TA-GroupAssignment", username = "dharanikiran", ref = "master",
subdir = NULL, port = NULL,
launch.browser = getOption("shiny.launch.browser", interactive()))
runGitHub("TA-GroupAssignment", username = "dharanikiran", ref = "master",
subdir = NULL, port = NULL,
launch.browser = getOption("shiny.launch.browser", interactive()))
runGitHub("TA-GroupAssignment","dharanikiran")
source("https://raw.githubusercontent.com/dharanikiran/TA-GroupAssignment/master/dependency.R")
runGitHub("TA-GroupAssignment", "dharanikiran")
runGitHub("TA-GroupAssignment", "dharanikiran")
runApp('GitHub/TA-GroupAssignment')
runApp('GitHub/TA-GroupAssignment')
shiny::runApp()
runApp()
runApp()

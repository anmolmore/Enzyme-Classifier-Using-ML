{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load job Clicks file into rdd\n",
    "ratings_raw_data = sc.textFile(\"file:///home/cloudera/job_clicks.csv\")\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "ratings_data = ratings_raw_data.filter(lambda line: line != ratings_raw_data_header).map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),int(float(tokens[2])))).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load jobs category file into rdd\n",
    "jobs_raw_data = sc.textFile(\"file:///home/cloudera/jobs.csv\" )\n",
    "jobs_raw_data_header = jobs_raw_data.take(1)[0]\n",
    "print (\"data size is \", ratings_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs_data = jobs_raw_data.filter(lambda line: line!=jobs_raw_data_header).map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1])).cache()\n",
    "print('Columns are:', jobs_raw_data_header)\n",
    "jobs_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into train, validation and test datasets\n",
    "rddTraining, rddValidating, rddTesting = ratings_data.randomSplit([6,2,2], seed=1001)\n",
    "\n",
    "nbValidating = rddValidating.count()\n",
    "nbTesting    = rddTesting.count()\n",
    "\n",
    "print(\"Training: %d, validation: %d, test: %d\" % (rddTraining.count(), nbValidating, rddTesting.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Training\n",
    "# Here, I am using RMSE but for these kind of problems where we have implicit features, it is better to use ** Mean Percentage Ranking (MPR) **\n",
    "\n",
    "def howFarAreWe(model, against, sizeAgainst):\n",
    "    againstNoRatings = against.map(lambda x: (int(x[0]), int(x[1])) )\n",
    "    againstWiRatings = against.map(lambda x: ((int(x[0]),int(x[1])), int(x[2])) )\n",
    "    predictions = model.predictAll(againstNoRatings).map(lambda p: ( (p[0],p[1]), p[2]) )\n",
    "    predictionsAndRatings = predictions.join(againstWiRatings).values()    \n",
    "    return sqrt(predictionsAndRatings.map(lambda s: (s[0] - s[1]) ** 2).reduce(add) / float(sizeAgainst))\n",
    "\n",
    "#finding best set of parameters\n",
    "ranks  = [5,10]\n",
    "reguls = [0.1, 1]\n",
    "iters  = [5]\n",
    "alpha = [10]\n",
    "\n",
    "finalModel = None\n",
    "finalRank  = 0\n",
    "finalRegul = float(0)\n",
    "finalIter  = -1\n",
    "finalDist   = float(300)\n",
    "finalAlpha = float(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[START train_model]\n",
    "for cRank, cRegul, cIter, cAlpha in itertools.product(ranks, reguls, iters, alpha):\n",
    "    model = ALS.trainImplicit(rddTraining, cRank, cIter, float(cRegul),alpha=float(cAlpha))\n",
    "    dist = howFarAreWe(model, rddValidating, nbValidating)\n",
    "    if dist < finalDist:\n",
    "        print(cIter, cRank,cAlpha,cRegul)\n",
    "        print(\"Best so far:%f\" % dist)\n",
    "        finalModel = model\n",
    "        finalRank  = cRank\n",
    "        finalRegul = cRegul\n",
    "        finalIter  = cIter\n",
    "        finalDist  = dist\n",
    "        finalAlpha  = cAlpha \n",
    "\n",
    "print(\"Rank %i\" % finalRank) \n",
    "print(\"Regul %f\" % finalRegul) \n",
    "print(\"Iter %i\" % finalIter)  \n",
    "print(\"Dist %f\" % finalDist) \n",
    "print(\"Alpha %f\" % finalAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model building with best set of parameters and predicting on test set\n",
    "\n",
    "model = ALS.trainImplicit(rddTraining, rank=finalRank, iterations=finalIter, lambda_= float(finalRegul),alpha=float(finalAlpha))\n",
    "# Calculate all predictions\n",
    "rddTesting_withoutclicks = rddTesting.map(lambda r: ((r[0], r[1])))\n",
    "predictions = model.predictAll(rddTesting_withoutclicks).map(lambda r: ((r[0], r[1]), (r[2])))\n",
    "predictions.take(3)\n",
    "# user id, node_id, actual clickss,pred clickss -> df below\n",
    "rates_and_preds = rddTesting.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions) \n",
    "rates_and_preds.take(3)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is %s' % (error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rates_and_preds.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "x = rates_and_preds.map(lambda x : (x[0][0],x[0][1],x[1][0],x[1][1]))\n",
    "hasattr(x, \"toDF\")\n",
    "x.toDF().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get clicks (total and average) for each job by users\n",
    "def get_counts_and_averages(ID_and_ratings_tuple):    \n",
    "    nratings = len(ID_and_ratings_tuple[1]) \n",
    "    return ID_and_ratings_tuple[0], (nratings, sum([float(val) for val in ID_and_ratings_tuple[1]])/nratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_ID_with_ratings_RDD = (ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "job_ID_with_ratings_RDD_updated = job_ID_with_ratings_RDD.map(lambda x : (x[0], list(x[1])))\n",
    "job_ID_with_avg_ratings_RDD = job_ID_with_ratings_RDD_updated.map(get_counts_and_averages)  # count and average rating\n",
    "job_rating_counts_RDD = job_ID_with_avg_ratings_RDD.map(lambda x: (int(x[0]), x[1][0]))    # rating count per job\n",
    "job_rating_counts_RDD.cache()\n",
    "job_rating_counts_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get user-wise jobs clicked\n",
    "all_users_ratings_RDD = ratings_data.map(lambda x: (x[0], x[1])).groupByKey()\n",
    "all_users_ratings_RDD = all_users_ratings_RDD.map(lambda x : (x[0], list(x[1])))    # jobs clicked by each user\n",
    "\n",
    "### finding unrated jobs by each user- we will use this set for model's prediction/recommendations\n",
    "job_ids = set(jobs_data.map(lambda x : x[0]).toLocalIterator()) # list of all job ids\n",
    "unrated_jobs_RDD = all_users_ratings_RDD.map(lambda x: (x[0], list((job_ids) - set(x[1]))))\n",
    "\n",
    "# #create user_id and unrated job id pairs\n",
    "unrated_userjobs_RDD = unrated_jobs_RDD.flatMap(lambda x : [(x[0],i) for i in x[1]])\n",
    "\n",
    "# # #model predictions for each user and not clicked job pairs\n",
    "recommendations_RDD = model.predictAll(unrated_userjobs_RDD)\n",
    "recommended_jobs_rating_RDD = recommendations_RDD.map(lambda x: (x.product,(x.user, x.rating)))\n",
    "recommended_jobs_rating_RDD.cache()\n",
    "print (recommended_jobs_rating_RDD.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join job title and clicks received for further filtering and recommendations\n",
    "# #     # converting id into int for job_clicks_count RDD to perform join\n",
    "# job_clicks_counts_RDD_updated = job_clicks_counts_RDD.map(lambda x: (int(x[0]), x[1]))\n",
    "\n",
    "# join job name with job id, predicted rating for job and total number of ratings received by each job\n",
    "recommendations_rating_title_and_count_RDD = recommended_jobs_rating_RDD.join(jobs_data).join(job_rating_counts_RDD)\n",
    "recommendations_rating_title_and_count_RDD = recommendations_rating_title_and_count_RDD.map(lambda r: (r[0], r[1][0][1], r[1][0][0][0],round(r[1][0][0][1],2),r[1][1]))\n",
    "recommendations_rating_title_and_count_RDD = recommendations_rating_title_and_count_RDD.map(lambda x: (x[2],(x[0],x[1], x[3],x[4])))\n",
    "recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Top recommendations\n",
    "# filter only those jobs which have been clicked by atleast 20 users\n",
    "# take only top5 jobs by sorting based on preference confidence\n",
    "top_jobs = recommendations_rating_title_and_count_RDD.groupBy(lambda x : x[0])\\\n",
    "                               .map(lambda x : list(x[1]))\\\n",
    "                               .map(lambda r: [i for i in r if i[1][3] > 20])\\\n",
    "                               .map(lambda a: [i for i in sorted(a, key=lambda x: -x[1][2])[:5]])   \n",
    "\n",
    "#preparing dataframe to insert in Database\n",
    "rec_jobs_df = top_jobs.map(lambda x: [(i[0],i[1][0],i[1][1],i[1][2]) for i in x]).flatMap(lambda x: x).toDF()\\\n",
    "                                .withColumnRenamed(\"_1\", \"user_id\")\\\n",
    "                                .withColumnRenamed(\"_2\", 'job_recommendations')\\\n",
    "                                .withColumnRenamed(\"_3\", 'job_category')\\\n",
    "                                .withColumnRenamed(\"_4\", 'preference_confidence')\\\n",
    "                                .withColumnRenamed(\"_5\", \"total_clicks\")\n",
    "                \n",
    "# #final recommendation engine dataframe to be saved in Database\n",
    "final_df_rec_eng = rec_jobs_df.withColumn(\"rec_date\", sf.lit(datetime.datetime.now()).cast(TimestampType()))   \n",
    "final_df_rec_eng = final_df_rec_eng.withColumn(\"rec_number\", sf.row_number().over(Window.partitionBy(\"user_id\").orderBy(desc(\"preference_confidence\"))))    \n",
    "final_df_rec_eng.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda2]",
   "language": "python",
   "name": "Python [anaconda2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

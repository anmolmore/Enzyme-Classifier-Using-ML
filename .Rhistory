plot(p)
max_center_nodes <- 4
max_vertices <- 5
distill.cog = function(dtm) {
dtm_matrix = as.matrix(dtm)  #convert dtm to matrix
adj.mat = t(dtm_matrix) %*% dtm_matrix   #Transpose the matrix
diag(adj.mat) = 0     #remove self word references from matrix
col_sum = order(apply(adj.mat, 2, sum), decreasing = T)  #order by sum
sum_matrix = as.matrix(adj.mat[col_sum[1:50], col_sum[1:50]])
a = colSums(sum_matrix) # get sum in vector
b = order(-a) #arrange in descending order
row_col_matrix = sum_matrix[b, b]  #create matrix with rows and columns
diag(row_col_matrix) =  0
wc = NULL
for (i in 1:max_center_nodes) {
thresh1 = row_col_matrix[i,][order(-row_col_matrix[i, ])[max_vertices]]
row_col_matrix[i, row_col_matrix[i,] < thresh1] = 0
row_col_matrix[i, row_col_matrix[i,] > 0 ] = 1
word = names(mat2[i, row_col_matrix[i,] > 0])
row_col_matrix[(i+1):nrow(row_col_matrix), match(word,colnames(row_col_matrix))] = 0
wc = c(wc, word)
}
row_col_matrix1 = row_col_matrix[match(wc, colnames(row_col_matrix)), match(wc, colnames(row_col_matrix))]
ord = colnames(row_col_matrix)[which(!is.na(match(colnames(row_col_matrix), colnames(row_col_matrix1))))]  #remove NA rows
row_col_matrix2 = row_col_matrix1[match(ord, colnames(row_col_matrix1)), match(ord, colnames(row_col_matrix1))]
return (row_col_matrix2)
}
dtm_plot <-  distill.cog(swiggy_dtm)
knitr::opts_chunk$set(echo = TRUE)
if (!require(tidytext)) {install.packages("tidytext")}
if (!require(tibble)) {install.packages("tibble")}
if (!require(tidyverse)) {install.packages("tidyverse")}
if (!require(wordcloud)) {install.packages("wordcloud")}
if (!require(ggplot2)) {install.packages("ggplot2")}
if (!require(igraph)) {install.packages("igraph")}
swiggy <- readLines('data/swiggy.txt')
zomato <- readLines('data/zomato.txt')
build_dtm <- function(corpus) {
df = data_frame(text = corpus)  #create dataframe
df_tokens = df %>%
mutate(doc = row_number()) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
group_by(doc) %>%
count(word, sort=TRUE)
df_counts = df_tokens %>% rename(value = n)
dtm = df_counts %>% cast_sparse(doc, word, value)
# order rows and colms putting max mass on the top-left corner of the DTM
colsum = apply(dtm, 2, sum)
col.order = order(colsum, decreasing=TRUE)
row.order = order(rownames(dtm) %>% as.numeric())
dtm1 = dtm[row.order, col.order]
return(dtm1)
}
swiggy_dtm = build_dtm(swiggy)
zomato_dtm = build_dtm(zomato)
build_wordcloud <- function(dtm) {
if (ncol(dtm) > 20000) {
chunk = round(ncol(dtm)/100)
a = rep(chunk,99)
b = cumsum(a)
rm(a)
b = c(0,b,ncol(dtm))
ss.col = c(NULL)
for (i in 1:(length(b)-1)) {
tempdtm = dtm[,(b[i]+1):(b[i+1])]
s = colSums(as.matrix(tempdtm))
ss.col = c(ss.col,s)
} # i loop ends
tsum = ss.col
}
else {
tsum = apply(dtm, 2, sum)
}
tsum = tsum[order(tsum, decreasing = T)]
return (tsum)
}
tsum <- build_wordcloud(swiggy_dtm)
wordcloud(names(tsum), tsum,     # words, their freqs
scale = c(3.5, 0.5),     # range of word sizes
5,                     # min.freq of words to consider
max.words = 150,       # max #words
colors = brewer.pal(8, "Dark2"))    # Plot results in a word cloud
title(sub = "Swiggy Tweets Word Cloud")     # title for the wordcloud display
plot.barchart <- function(dtm) {
a0 = apply(dtm, 2, sum)
a1 = order(a0, decreasing = TRUE)
tsum = a0[a1]
return (tsum)
}
# plot barchart for top tokens
tsum <- plot.barchart(swiggy_dtm)
test = as.data.frame(round(tsum[1:15],0))  #max words to plot
p = ggplot(test, aes(x = rownames(test), y = test[,])) +
geom_bar(stat = "identity", fill = "Brown") +
geom_text(aes(label = test[,]), vjust= -0.20) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
plot(p)
tsum <- plot.barchart(zomato_dtm)
test = as.data.frame(round(tsum[1:15],0))  #max words to plot
p = ggplot(test, aes(x = rownames(test), y = test[,])) +
geom_bar(stat = "identity", fill = "Brown") +
geom_text(aes(label = test[,]), vjust= -0.20) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
plot(p)
max_center_nodes <- 4
max_vertices <- 5
distill.cog = function(dtm) {
dtm_matrix = as.matrix(dtm)  #convert dtm to matrix
adj.mat = t(dtm_matrix) %*% dtm_matrix   #Transpose the matrix
diag(adj.mat) = 0     #remove self word references from matrix
col_sum = order(apply(adj.mat, 2, sum), decreasing = T)  #order by sum
sum_matrix = as.matrix(adj.mat[col_sum[1:50], col_sum[1:50]])
a = colSums(sum_matrix) # get sum in vector
b = order(-a) #arrange in descending order
row_col_matrix = sum_matrix[b, b]  #create matrix with rows and columns
diag(row_col_matrix) =  0
word_count = NULL
for (i in 1:max_center_nodes) {
thresh1 = row_col_matrix[i,][order(-row_col_matrix[i, ])[max_vertices]]
row_col_matrix[i, row_col_matrix[i,] < thresh1] = 0
row_col_matrix[i, row_col_matrix[i,] > 0 ] = 1
word = names(row_col_matrix[i, row_col_matrix[i,] > 0])
row_col_matrix[(i+1):nrow(row_col_matrix), match(word,colnames(row_col_matrix))] = 0
word_count = c(word_count, word)
}
row_col_matrix1 = row_col_matrix[match(word_count, colnames(row_col_matrix)), match(word_count, colnames(row_col_matrix))]
order = colnames(row_col_matrix)[which(!is.na(match(colnames(row_col_matrix), colnames(row_col_matrix1))))]  #remove NA rows
row_col_matrix2 = row_col_matrix1[match(order, colnames(row_col_matrix1)), match(order, colnames(row_col_matrix1))]
return (row_col_matrix2)
}
dtm_plot <-  distill.cog(swiggy_dtm)
graph <- graph.adjacency(dtm_plot, mode = "undirected", weighted=T) #Create Network object
graph <- simplify(graph)
V(graph)$color[1:max_center_nodes] = "green" #central node color
V(graph)$color[max_center_nodes+1:length(V(graph))] = "pink"  #vertex colors
graph = delete.vertices(graph, V(graph)[ degree(graph) == 0 ]) #delete empty vertices
plot(graph, layout = layout.kamada.kawai, main = "Swiggy COG")
dtm_plot <-  distill.cog(zomato_dtm)
graph <- graph.adjacency(dtm_plot, mode = "undirected", weighted=T) #Create Network object
graph <- simplify(graph)
V(graph)$color[1:4] <- "green" #central node color
V(graph)$color[5:length(V(graph))] <- "pink"  #vertex colors
graph = delete.vertices(graph, V(graph)[ degree(graph) == 0 ]) #delete empty vertices
plot(graph, layout = layout.kamada.kawai, main = "Zomato COG")
if (!require(syuzhet)) {install.packages("syuzhet")}
if (!require(ggplot2)) {install.packages("ggplot2")}
if (!require(matrixStats)) {install.packages("matrixStats")}
swiggy = readLines('data/swiggy.txt')
sentiment <- get_nrc_sentiment((swiggy))
sentiment_scores <- data.frame(colMeans(sentiment[,]))
names(sentiment_scores) <- "Score"
sentiment_scores <- cbind("sentiment" = rownames(sentiment_scores), sentiment_scores)
rownames(sentiment_scores) <- NULL
#plotting the sentiments with scores
ggplot(data=sentiment_scores, aes(x=sentiment, y=Score)) +
geom_bar(aes(fill=sentiment), stat = "identity") +
theme(legend.position = "none")+
ylim(0,1.2) +
xlab("Sentiment Type") + ylab("Mean sentiment score") + ggtitle("Sentiments of people tweeting to Swiggy handle")
zomato = readLines('data/zomato.txt')
sentiment <- get_nrc_sentiment((zomato))
sentiment_scores <- data.frame(colMeans(sentiment[,]))
#sentiment_scores <- data.frame(colSums(sentiment[,]))
names(sentiment_scores) <- "Score"
sentiment_scores <- cbind("sentiment" = rownames(sentiment_scores), sentiment_scores)
rownames(sentiment_scores) <- NULL
#plotting the sentiments with scores
ggplot(data=sentiment_scores, aes(x=sentiment, y=Score)) +
geom_bar(aes(fill=sentiment), stat = "identity") +
theme(legend.position = "none")+
ylim(0,1.2) +
xlab("Sentiment Type") + ylab("Mean sentiment score") + ggtitle("Sentiments of people tweeting to Zomato handle")
if (!require(tidytext)) {install.packages("tidytext")}
if (!require(ggplot2)) {install.packages("ggplot2")}
if (!require(dplyr)) {install.packages("dplyr")}
zomato <- readLines('data/zomato.txt')
swiggy <- readLines("data/swiggy.txt")
get_positive_negative_word_counts <- function(corpus) {
df = data_frame(text = zomato)  #create dataframe from corpus
bing_sentiments = get_sentiments("bing")
word_counts <- df %>%
unnest_tokens(word, text) %>%
inner_join(bing_sentiments) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
return (word_counts)
}
swiggy_word_counts <- get_positive_negative_word_counts(swiggy)
swiggy_word_counts %>%
filter(n > 20) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
xlab("Words") +
ylab("Count of words") +
ggtitle("Words contributing towards Swiggy sentiment")
zomato_word_counts <- get_positive_negative_word_counts(zomato)
zomato_word_counts %>%
filter(n > 20) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
xlab("Words") +
ylab("Count of words") +
ggtitle("Words contributing towards Zomato sentiment")
setwd("~/OneDrive - Indian School of Business/Genomic/genomics")
# download the e1071 library
install.packages("e1071")
# download the SparseM library
install.packages("SparseM")
# load the libraries
library(e1071)
library(SparseM)
# load the csv dataset into memory
train <- read.csv('final_data/protien-sequences.csv')
View(train)
View(train)
# put the labels in a separate vector
y <- train[,'type_numeric']
features_data <- data %>% select('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',
'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')
library(dpylr)
library(dplyr)
# load the csv dataset into memory
data <- read.csv('final_data/protien-sequences.csv')
features_data <- data %>% select('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',
'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')
x <- data.matrix(features_data)
View(x)
# put the labels in a separate vector
y <- train[,'type_numeric']
# convert to compressed sparse row format
xs <- as.matrix.csr(x)
# write the output libsvm format file
write.matrix.csr(xs, y=y, file="out.txt")
# write output to libsvm format
write.matrix.csr(x_matrix, y=y, file="data.txt")
# download e1071 library if not available
if (!require(e1071)) {install.packages("e1071")}
# download sparseM library if not available
install.packages("SparseM")
# load the libraries
library(e1071)
library(SparseM)
# load the csv dataset into memory
data <- read.csv('final_data/protien-sequences.csv')
# take the numeric columns are format as matrix
x <- as.matrix(data[,('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',
'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# assign labels to vector y
y <- data[,15]
# convert input columns to sparse matrix
x_matrix <- as.matrix.csr(x)
# write output to libsvm format
write.matrix.csr(x_matrix, y=y, file="data.txt")
View(data)
# take the numeric columns are format as matrix
x <- as.matrix(data[,('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',
'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# take the numeric columns are format as matrix
x <- as.matrix(data[('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',
'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# take the numeric columns are format as matrix
x <- as.matrix(data[,('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',
'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# assign labels to vector y
y <- data[,15]
# take the numeric columns are format as matrix
x <- as.matrix(data[,('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',
'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# Ref : https://vatsalbits.wordpress.com/2016/01/13/csv-to-libsvm/
# download e1071 library if not available
if (!require(e1071)) {install.packages("e1071")}
# download sparseM library if not available
if (!require(SparseM)) {install.packages("SparseM")}
# load the libraries
library(e1071)
library(SparseM)
# load the csv dataset into memory
data <- read.csv('final_data/protien-sequences.csv')
# take the numeric columns are format as matrix
x <- as.matrix(data[,('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# assign labels to vector y
y <- data[,15]
# convert input columns to sparse matrix
x_matrix <- as.matrix.csr(x)
# write output to libsvm format
write.matrix.csr(x_matrix, y=y, file="data.txt")
# take the numeric columns are format as matrix
x <- as.matrix(data[,c('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
View(x)
# assign labels to vector y
y <- data[,'type_numeric']
# Ref : https://vatsalbits.wordpress.com/2016/01/13/csv-to-libsvm/
# download e1071 library if not available
if (!require(e1071)) {install.packages("e1071")}
# download sparseM library if not available
if (!require(SparseM)) {install.packages("SparseM")}
# load the libraries
library(e1071)
library(SparseM)
# load the csv dataset into memory
data <- read.csv('final_data/protien-sequences.csv')
# take the numeric columns are format as matrix
x <- as.matrix(data[,c('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# assign labels to vector y
y <- data[,'type_numeric']
# convert input columns to sparse matrix
x_matrix <- as.matrix.csr(x)
# write output to libsvm format
write.matrix.csr(x_matrix, y=y, file="data.txt")
# Ref : https://vatsalbits.wordpress.com/2016/01/13/csv-to-libsvm/
# download e1071 library if not available
if (!require(e1071)) {install.packages("e1071")}
# download sparseM library if not available
if (!require(SparseM)) {install.packages("SparseM")}
# load the libraries
library(e1071)
library(SparseM)
# load the csv dataset into memory
data <- read.csv('final_data/protien-sequences.csv')
# take the numeric columns are format as matrix
x <- as.matrix(data[,c('A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'gravy', 'weight')])
# assign labels to vector y
y <- data[,'type_numeric']
# convert input columns to sparse matrix
x_matrix <- as.matrix.csr(x)
# write output to libsvm format
write.matrix.csr(x_matrix, y=y, file="data/protien-sequences-libsvm.txt")
knitr::opts_chunk$set(echo = TRUE)
if (!require(openNLP)) {install.packages("openNLP")}
View(data)
if (!require(readtext)) {install.packages("readtext")}
require(openNLP)
require(tm)
require(NLP)
library(readtext)
library(dplyr)
library(stringr)
library(tidyverse)
library(tidytext)
get_sentences_from_corpus <- function(text, lang = "en") {
#use openNLP sentence tokenizer
sentence_tokenizer <- Maxent_Sent_Token_Annotator(language = lang)
text <- as.String(text)  #convert text to string
sentence_list <- annotate(text, sentence_tokenizer) #look for sentence boundaries
sentences <- text[sentence_list]  #get sentences
# return sentences
return(sentences)
}
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
colnames(df) <- "Content"
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
i <- i+1
}
knitr::opts_chunk$set(echo = TRUE)
if (!require(openNLP)) {install.packages("openNLP")}
if (!require(readtext)) {install.packages("readtext")}
require(openNLP)
require(tm)
require(NLP)
library(readtext)
library(dplyr)
library(stringr)
library(tidyverse)
library(tidytext)
get_sentences_from_corpus <- function(text, lang = "en") {
#use openNLP sentence tokenizer
sentence_tokenizer <- Maxent_Sent_Token_Annotator(language = lang)
text <- as.String(text)  #convert text to string
sentence_list <- annotate(text, sentence_tokenizer) #look for sentence boundaries
sentences <- text[sentence_list]  #get sentences
# return sentences
return(sentences)
}
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
colnames(df) <- "Content"
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
i <- i+1
}
View(files)
View(master_df)
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
#colnames(df) <- "Content"
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
i <- i+1
}
knitr::opts_chunk$set(echo = TRUE)
if (!require(openNLP)) {install.packages("openNLP")}
if (!require(readtext)) {install.packages("readtext")}
require(openNLP)
require(tm)
require(NLP)
library(readtext)
library(dplyr)
library(stringr)
library(tidyverse)
library(tidytext)
get_sentences_from_corpus <- function(text, lang = "en") {
#use openNLP sentence tokenizer
sentence_tokenizer <- Maxent_Sent_Token_Annotator(language = lang)
text <- as.String(text)  #convert text to string
sentence_list <- annotate(text, sentence_tokenizer) #look for sentence boundaries
sentences <- text[sentence_list]  #get sentences
# return sentences
return(sentences)
}
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
#colnames(df) <- "Content"
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
i <- i+1
}
if (!require(openNLP)) {install.packages("openNLP")}
if (!require(readtext)) {install.packages("readtext")}
require(openNLP)
require(tm)
require(NLP)
library(readtext)
library(dplyr)
library(stringr)
library(tidyverse)
library(tidytext)
get_sentences_from_corpus <- function(text, lang = "en") {
#use openNLP sentence tokenizer
sentence_tokenizer <- Maxent_Sent_Token_Annotator(language = lang)
text <- as.String(text)  #convert text to string
sentence_list <- annotate(text, sentence_tokenizer) #look for sentence boundaries
sentences <- text[sentence_list]  #get sentences
# return sentences
return(sentences)
}
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
#colnames(df) <- "Content"
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
i <- i+1
}
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
print(df)
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
i <- i+1
}
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
print(df)
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
i <- i+1
}
wiki_files = c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
sentences <- get_sentences_from_corpus(file)
df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
print(df)
df['Category'] <- wiki_files[i]
master_df <- rbind(master_df, df)
}
sentences <- get_sentences_from_corpus(file[1])

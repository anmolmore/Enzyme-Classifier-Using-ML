---
title: "Introductory Text-An with Tidytext"
author: Sudhir Voleti
output:
  html_document: default
  html_notebook: default
---

### Basic Intro to Tidytext

Tidytext is text-an part of 'tidyverse' - a set of  popular packages that work on a common set of code principles called the ['tidy data framework'](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) originally created by the legendary (in R circles, at least) Hadley Wickham. 

We'll see some of these tidy data handling procedures as we proceed ahead. Tidytext (Silge and Robinson 2016) is described thus by its creators: 

>"Treating text as data frames of individual words allows us to manipulate, summarize, and visualize the characteristics of text easily and integrate natural language processing into effective workflows we were already using."

So let's jump right in.

```{r setup}


if (!require(tidyverse)) {install.packages("tidyverse")}
if (!require(tidytext)) {install.packages("tidytext")}

library(tidyverse)
library(tidytext)
library(ggplot2)


```
### Tidytext Analysis on the Nokia dataset

I return to an old favorite here: the 2013 Amazon Nokia Lumia reviews dataset of 120 consumer reviews.  

One advantage is that I can read the data directly off my github page.

```{r nokia_basic}
# Example reading in an amazon nokia corpus
nokia = readLines('https://github.com/sudhir-voleti/sample-data-sets/raw/master/text%20analysis%20data/amazon%20nokia%20lumia%20reviews.txt')
# a0 = readLines(file.choose())    # alternately, do this to read file from local machine

text <- nokia
text  =  gsub("<.*?>", " ", text)              # regex for removing HTML tags
length(text)    # 120 documents
```

### Tidytext Tokenization with unnest_tokens()

First thing to note before invoking tidytext - get the text into a `data_frame` format, called a `tibble`, which has nice printing properties - e.g., it never floods the console.  

In other words, we convert text_df into one-token-per-document-per-row.   

Now time to see tidytext's neat tokenizing capabilities at the level of:  

+  (i) words, 
+  (ii) sentences, 
+  (iii) paragraphs, and 
+  (iv) ngrams, 

using the `unnest_tokens()` function.

```{r tokenizing_nokia}
require(tibble)
textdf = tibble(text = text) # yields 120x1 tibble. i.e., each doc = 1 row here.
  # textdf # commenting this out as it is messing up page formatting

# Tokenizing ops. Words first.
system.time({
textdf %>% unnest_tokens(word, text)   # try ?unnest_tokens
})
```
So, how many words are there in the Nokia corpus?  

Below, we can see sentence tokenization using the `token = "sentences"` parm in `unnest_tokens`.

```{r nokia_senten}
# Tokenizing into sentences.
system.time({
sent_tokenized = textdf %>% unnest_tokens(sentence, text, token = "sentences") # 868 rows/120 docs ~  8 sentences per doc?
  })   # 0 secs

# explore output object a bit
class(sent_tokenized)
dim(sent_tokenized)
head(sent_tokenized)
```
Sentence detection appears to be good, above. Tokenizing df into bigrams, below. Try different ngrams & see.

```{r nokia_bigrm}
system.time({
ngram_tokenized = textdf %>% unnest_tokens(ngram, text, token = "ngrams", n = 2)     # yields (#tokens -1 ) bigrams
})  # 0.03 secs

#explore the output object
class(ngram_tokenized)
dim(ngram_tokenized)
head(ngram_tokenized)
```
### Grouping, Aggegation and Join ops in tidytext

Intuitively, the funcs we'll call in tidytext are named `group_by()`, `count()` and `join()`. Behold.

```{r}
# use count() to see most common words
system.time({
nokia_words = textdf %>% 
        unnest_tokens(word, text) %>%  # tokenized words in df in 'word' colm
        count(word, sort = TRUE) %>%   # counts & sorts no. of occurrences of each item in 'word' column 
        rename(count = n)      # renames the count column from 'n' (default name) to 'count'.
  }) # 0.11 secs       

nokia_words %>% head(., 10) # view top 10 rows in nokia-words df
```
Unsurprisingly, the most common words, with the highest no. of occurrences are th scaffolding of grammer - articles, prepositions, conjunctions etc. Not exactly the meaty stuff.

Is there anything we can do about it? Yes, we can. (Pun intended). We could *filter* the words using a *stopwords* list from earlier.

Conveniently, tidytext comes with its own `stop_words` list. 

```{r anti_join ops}
data(stop_words)

# use anti_join() to de-merge stopwords from the df
system.time({

  nokia_new = textdf %>% 
        unnest_tokens(word, text) %>% 
        count(word, sort = TRUE) %>%   
        rename(count = n) %>%
        anti_join(stop_words)   # try ?anti_join

  })  # 0.03 secs

  nokia_new %>% head(., 10)
```
Aha. With the stopwords filtered out, the scene changes quite a bit.

How about some visualization via bar-charts, just for completeness? See below.

```{r}
# First, build a datafame
tidy_nokia <- textdf %>% 
  unnest_tokens(word, text) %>%     # word tokenization 
  anti_join(stop_words)    # run ?join::dplyr 

# Visualize the commonly used words using ggplot2.
library(ggplot2)

tidy_nokia %>%
  count(word, sort = TRUE) %>%
  filter(n > 20) %>%   # n is wordcount colname. 
  
  mutate(word = reorder(word, n)) %>%  # mutate() reorders columns & renames too
  
  ggplot(aes(word, n)) +
  geom_bar(stat = "identity", col = "red", fill = "red") +
  xlab(NULL) +
  coord_flip()
```
### More Bigram ops with tidytext

Bigrams in particular, and ngrams in general will have several uses down the line, as we'll see. As such they link BOW with NLP in a way.

First, let's build and view the bigrams with `token = "ngrams, n=2` argument in the `unnest_tokens()` func.

```{r}
nokia_bigrams <- textdf %>%
  unnest_tokens(bigram, text, 
                token = "ngrams", n = 2)
nokia_bigrams
```
Now use `separate()` to str_split() the words.

```{r}
require(tidyr)
# separate bigrams
bigrams_separated <- nokia_bigrams %>%
                        separate(bigram, c("word1", "word2"), sep = " ") 
bigrams_separated
```
Very many stopwords in there obscuring meaning. So let's remove them by successively doing `filter()` on the first and second words.

```{r}
# filtering the bigrams to remove stopwords
bigrams_filtered <- bigrams_separated %>%
                      filter(!word1 %in% stop_words$word) %>%
                      filter(!word2 %in% stop_words$word)

bigrams_filtered
```
Other ops like counting and sorting should be standard by now.

```{r}
# New bigram counts:
bigram_counts <- bigrams_filtered %>% 
        count(word1, word2, sort = TRUE)

bigram_counts
```
tidyr's `unite()` function is the inverse of separate(), and lets us recombine the columns into one. 

Thus, "separate / filter / count / unite" functions let us find the most common bigrams not containing stop-words.

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
bigrams_united
```
Suppose we want to identify all those bigrams wherein the words "cloud", "windows" or "solution" appeared either in word1 or in word2.

In other words, we want to match arbitrary word strings with bigram components. See code below.

```{r}
# filter for bigrams which contain the word game, intelligent, or cloud
arbit_filter = bigrams_filtered %>%
			filter(word1 %in% c("windows", "solution", "cloud") | word2 %in% c("windows", "solution", "cloud")) %>%
      count(word1, word2, sort = TRUE)

arbit_filter %>% filter(word1 == "windows" | word2 == "cloud")   # try for cloud in word2 etc.
```

### Casting tidy data into DTMs

Nice that tidy data can do so much. But how about building standard text-an data objects like DTMs etc?

Sure, tidytext can very well do that. First, we convert the text corpus into the tidy format, essentially, one-token-per-row-per-document. See the code below.

```{r nokia_dtm}
# First convert Nokia corpus to tidy format, i.e. a tibble with token and count per row per document.

tidy_nokia = textdf %>%   
                    mutate(doc = row_number()) %>%
                    unnest_tokens(word, text) %>% 
                    anti_join(stop_words) %>%
                    group_by(doc) %>%
                    count(word, sort=TRUE)
tidy_nokia
```
Now, to cast this tidy object into a DTM, into a regular (if sparse) matrix etc. See code below.

```{r}
# cast into a Document-Term Matrix
nokia_dtm = tidy_nokia %>%
            cast_dtm(doc, word, n)

nokia_dtm   # yields a sparse matrix format by default
```
Below, we convert `tidy_nokia` into a regular matrix.

```{r}
# cast into a Matrix object
nokia_dtm <- tidy_nokia %>% cast_sparse(doc, word, n)
    # class(nokia_dtm)  # Matrix

nokia_dtm[1:6,1:6]
```
### the TFIDF transformation

Recall we mentioned there're two weighing schemes for token frequencies in the DTM. 

What we saw above was the simpler, first one, namely, term frequency or TF.

Now we introduce the second one - TFIDF. Which stands for term frequency-inverse document frequency.

Time to move to the slides and the board before returning here. Tidytext code for the TFIDF transformation is below.

P.S. Let $tf(term)$ denote the term frequncy of some term of interest. Then $tfidf(term)$ is defind as $\frac{tf(term)}{idf(term)}$

where $idf(term) = ln\frac{n_{documents}}{n_{documents.with.term}}$. Various other schemes have been proposed.

```{r nokia_idf}

 nokia_dtm_idf = tidy_nokia %>% 
		group_by(doc) %>% 
		count(word, sort=TRUE) %>% ungroup() %>%
		bind_tf_idf(word, doc, nn) %>% 
		cast_sparse(doc, word, tf_idf)

 nokia_dtm_idf[1:8, 1:8]   # view a few rows

```
Last but not least is the ability to explicitly leverage tidytext's tidy data format to do some cool stuff. Below is only if we have time.

### Applying Tidy Principles on Text data

The Nokia corpus has 120 documents. Each document has several sentences. And each sentence has several words in it. Here's a Q for you now.  

> Suppose you want to know how many sentences are there in each document in the corpus. And how many words are there in each sentence. How would you go about this?

`dplyr` functions inside tidytext get us there easily because the data are arranged as per tidy principles.

I present a few simple examples below. These operators while simple enable us to build fairly complex structures by recombining and chaining them together. 

Let's answer the question set above. I'll use functions `row_number()` and `group_by()`. See the code below.

```{r doc_id}
### Create a document id and group_by it
textdf_doc = textdf %>% 
              mutate(doc = row_number()) %>%   # creates an id number for each row, i.e., doc.
              select(doc, text)
              # group_by(doc)   # internally groups docs together

textdf_doc
```
So I have doc id and corresponding text. What I next need is a sentence identifier within each doc.   

Then, a word identifier for each word within each sentence. Finally, can simply merge the dataframes. See below.  

```{r senten_doc}
### How many sentences in each doc?
textdf_sent = textdf_doc %>% 
              unnest_tokens(sentence, text, token = "sentences") %>% 
  
              group_by(doc) %>%
              mutate(sent_id = row_number()) %>%      # create sentence id by enumeration
              summarise(sent_doc = max(sent_id)) %>%  # max sent_id is the num of sents in doc
  
              select(doc, sent_doc)    # retain colms in order for clean display

textdf_sent   # first few rows, view.
```
So, on average how many sentences does a product reviewer write on Amazon for this corpus? The above DF gives the answer. 

Next, to find average doc length, will need to know how many words are there in each document. See below.   

```{r words_per_doc}
### How many words in each document?
textdf_word = textdf_doc %>%   # using textdf_doc and not textdf_sent!
              unnest_tokens(word, text) %>% 
  
              group_by(doc) %>%
              mutate(word_id = row_number()) %>%
              summarise(word_doc = max(word_id)) %>%
              select(doc, word_doc)

textdf_word[1:10,]
```
The Qs continue. So I have textdf_sent that maps sentences to documents and textdf_word that maps wordcounts to document.

Can I **combine** the two dataframes? Yes we can. Try `?inner_join` and see.

```{r merge_ops}
### can we merge the above 2 tables together? 
# colnames(textdf_sent)   # doc, sent_doc
# colnames(textdf_word)   # doc, word_doc

doc_sent_word = inner_join(textdf_sent,   # first df
                           textdf_word,   # second df 
                           by = "doc") %>% # inner_join by 'doc' colm
  
			          mutate(words_per_sent = word_doc/sent_doc)   # avg no. of words per sentence as a new variable

doc_sent_word[1:10,]   # view first few rows of the new merged table
```
Neat, eh? One last thing. We've mapped sentences to documents and words to docs. Can we now map words to sentences?  

In other words, how many words are there per sentence? See below.

```{r}
### how many words in each sentence?
textdf_sent_word = textdf %>%   
                    unnest_tokens(sentence, text, token = "sentences") %>% 
                    mutate(senten = row_number()) %>% # building an index for sentences now
                    select(senten, sentence) %>%

			              unnest_tokens(word, sentence) %>% 
                    mutate(word_id = row_number()) %>%   # creating an intermed variable for word-counting
                    select(senten, word_id) %>%

			              group_by(senten) %>%  # grouping by sent id
                    summarise(words_in_sent = max(word_id)) %>% # summarizing
                    select(senten, words_in_sent)		# retain only relevant cols	

textdf_sent_word[11:20,]
```
Can we merge the document number also above? Yes. But I'll leave that as a practice exercise for you.

We now know enough to code basic text-an functions in tidytext. And further to head to sentiment-an in the next session.

Sudhir
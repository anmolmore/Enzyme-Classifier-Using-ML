---
title: "Topic Model Simulation"
output: word_document
---

### Team Members :

#### Anmol More : 11915043

#### Dharani Kiran Kavuri : 11915033

#### Shubhendu Vimal : 11915067

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### import libraries in specific order

```{r import libraries}
if (!require(NLP)) {install.packages("NLP")}
if (!require(tm)) {install.packages("tm")}
if (!require(openNLP)) {install.packages("openNLP")}
if (!require(readtext)) {install.packages("readtext")}
if (!require(dplyr)) {install.packages("dplyr")}
if (!require(stringr)) {install.packages("stringr")}
if (!require(tidytext)) {install.packages("tidytext")}

#order of libraries is important here to avoid masking of functions
require(NLP)
require(tm)
require(openNLP)
library(readtext)
library(dplyr)
library(stringr)
library(tidytext)
```

## Data Collection and cleaning

Collect data from wikipedia, through 'wikipedia' library in python, code at - Collect Wikipedia Data.ipynb

Ref : https://stackoverflow.com/questions/18712878/r-break-wiki_content-into-sentences

```{r get sentences, echo=FALSE}
get_sentences_from_wiki_content <- function(text, lang = "en") {
  #use openNLP sentence tokenizer
  sentence_tokenizer <- Maxent_Sent_Token_Annotator(language = lang)
  
  text <- as.String(text)  #convert text to string
  sentence_list <- annotate(text, sentence_tokenizer) #look for sentence boundaries
  sentences <- text[sentence_list]  #get sentences
  # return sentences
  return(sentences)
}
```

Take four different topics from wikipedia files

```{r prepare dataframe}
wiki_files <- c('Brexit.txt', 'Donald Trump.txt', 'Game of Thrones.txt', 'Bitcoin.txt')
categories <- c('Brexit', 'Donald Trump', 'Game of Thrones', 'Bitcoin')
files <- lapply(wiki_files, readLines)
master_df <- data.frame()
i <- 1
for (file in files) {
  sentences <- get_sentences_from_wiki_content(file)
  df <- data.frame(matrix(unlist(sentences), nrow=length(sentences), byrow=T), stringsAsFactors=FALSE)
  colnames(df) <- 'Content'
  df['Category'] <- categories[i]
  master_df <- rbind(master_df, df)
  i <- i+1
}

#random shuffling of dataframe rows
df <- master_df[sample(nrow(master_df)),]

```
### Document creation

Number the rows of dataframe, such that each document contains 8 lines

```{r document creation}
no_of_docs <- nrow(df)/8
df <- df[sample(nrow(df)),]
doc_sequences <- rep(1:no_of_docs, each=8)
df$doc_id <- doc_sequences
```

See how random our documents have been assigned across all 216 documents

```{r}
print(as.data.frame(table(df$Category)))
```

### Data cleaning

Remove all stop words, special characters, punctuations etc

Ref :

- https://stackoverflow.com/questions/48545106/using-tm-package-in-r-to-clean-the-columns-in-dataframe

- https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/

```{r data cleaning}
if (!require(tm)) {install.packages("tm")}
library(tm)

wiki_content <- Corpus(VectorSource(df$Content))

remove_symbols <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})
wiki_content <- tm_map(wiki_content, remove_symbols, "-")
wiki_content <- tm_map(wiki_content, remove_symbols, "=")

wiki_content <- tm_map(wiki_content, removeNumbers)
wiki_content <- tm_map(wiki_content, content_transformer(tolower))
wiki_content <- tm_map(wiki_content, removeWords, stopwords('english'))
wiki_content <- tm_map(wiki_content, removePunctuation)
wiki_content <- tm_map(wiki_content, stripWhitespace)

custom_stop_words <- c("trump")
wiki_content <- tm_map(wiki_content, removeWords, custom_stop_words)

clean_content <- unlist(as.list(wiki_content))
df$text_content <- clean_content
```

## Topic Modeling

### Sentence proportion matrix

Create sentence proportion matrix, calculating percentage of each topic in each document

```{r sentence prop matrix}
sentence_prop_matrix <- data.frame(matrix(ncol = 5, nrow = 0))
for (i in seq(1:no_of_docs)) {
  df_current_doc <- subset(df, df$doc_id == i)
  frequency_count <- as.data.frame(table(df_current_doc$Category))
  
  brexit <- ifelse(length(frequency_count[frequency_count$Var1 == "Brexit",]$Freq) > 0, 
                   frequency_count[frequency_count$Var1 == "Brexit",]$Freq, 0)
  trump <- ifelse(length(frequency_count[frequency_count$Var1 == "Donald Trump",]$Freq) > 0, 
                  frequency_count[frequency_count$Var1 == "Donald Trump",]$Freq, 0)
  got <- ifelse(length(frequency_count[frequency_count$Var1 == "Game of Thrones",]$Freq) > 0,
                frequency_count[frequency_count$Var1 == "Game of Thrones",]$Freq, 0)
  bitcoin <- ifelse(length(frequency_count[frequency_count$Var1 == "Bitcoin",]$Freq) > 0,
                    frequency_count[frequency_count$Var1 == "Bitcoin",]$Freq, 0)
  sentence_prop_matrix <- rbind(sentence_prop_matrix, 
                                data.frame('Document Id'=i,
                                           'Brexit' = brexit/8, 
                                           'Donald Trump' = trump/8,
                                           'Game of Thrones' = got/8,
                                           'Bitcoin' = bitcoin/8))
}
sentence_prop_matrix
```

### DTM creation

create count of words across each document

```{r}
df_by_words <- df %>%
  unnest_tokens(word, text_content)

df_word_count <- df_by_words %>% 
  anti_join(stop_words) %>%
  count(doc_id, word, sort = TRUE) %>%
  ungroup()

documents_dtm <- df_word_count %>%
  cast_dtm(doc_id, word, n)
```

### Run LDA

```{r lda}
library(topicmodels)

system.time({
  documents_lda <- LDA(documents_dtm, k = 4)
  })  
documents_lda
```
Ref - https://cran.r-project.org/web/packages/tidytext/vignettes/topic_modeling.html

Identify topics based on beta parameter to get basic idea

```{r}
library(tidytext)
term_topics <- tidy(documents_lda, matrix = "beta", control = list(seed = 9876))
term_topics
str(term_topics)
```

Identify topics based on gamma parameter to get basic idea
```{r}
top_terms = term_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

library(ggplot2)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

Lot of confusion is there between topic 3 and topic 4

```{r gamma matrix}
documents_gamma = tidy(documents_lda, matrix = "gamma")
documents_gamma
```

Based on above beta and gamma matrix, we see that
 
 * Trump - topic1
 
 * GOT - topic2
 
 * Brexit - topic4
 
 * Bitcoin - topic3
 
Create a gamma proportion matrix to do a comparision to our original sentence proportion matrix

```{r}
gamma_prop_matrix <- data.frame(matrix(ncol = 5, nrow = 0))
for (i in seq(1:(nrow(documents_gamma)/4))) {
  df_current_doc <- subset(documents_gamma, documents_gamma$document == i)
  prop_topic_1 <- df_current_doc[df_current_doc$topic == 1,]$gamma
  prop_topic_2 <- df_current_doc[df_current_doc$topic == 2,]$gamma
  prop_topic_3 <- df_current_doc[df_current_doc$topic == 3,]$gamma
  prop_topic_4 <- df_current_doc[df_current_doc$topic == 4,]$gamma
  
  gamma_prop_matrix <- rbind(gamma_prop_matrix, 
                                data.frame('Document Id'=i,
                                           'Brexit' = prop_topic_4, 
                                           'Donald Trump' = prop_topic_1,
                                           'Game of Thrones' = prop_topic_2,
                                           'Bitcoin' = prop_topic_3))
}
gamma_prop_matrix
```

## Create confusion matrix

Check using confusion matrix accuracy of our topic modeling, steps -

* Indentify highest probability as per gamma matrix for each document

* Indentify highest proportion of sentences in each of our document initially created

* Create a comparision matrix

```{r}

topics <- c('Brexit', 'Donald.Trump', 'Game.of.Thrones', 'Bitcoin')

gamma_prop_matrix$highest_gamma <- colnames(gamma_prop_matrix[topics])[max.col(gamma_prop_matrix[topics],ties.method="first")]

sentence_prop_matrix$highest_proportion <- colnames(sentence_prop_matrix[topics])[max.col(sentence_prop_matrix[topics],ties.method="first")]

confusion_matrix <- data.frame(matrix(ncol = 4, nrow = 0))

for (topic in topics) {
  #print(paste0("Identifying accuracy for  : ",topic))
  accurate_counts <- 0
  error_counts <- 0
  sentence_prop_matrix_subset <- subset(sentence_prop_matrix, sentence_prop_matrix$highest_proportion == topic)
  for (i in (1:nrow(sentence_prop_matrix_subset))) {
    document_id <- sentence_prop_matrix_subset[i, ]$Document.Id
    if(gamma_prop_matrix[gamma_prop_matrix$Document.Id == document_id,]$highest_gamma == topic) {
      accurate_counts <- accurate_counts + 1
    }
    else {
      error_counts <- error_counts + 1
    }
  }
  confusion_matrix <- rbind(confusion_matrix, 
                                data.frame('Topic'=topic,
                                           'Accurate' = accurate_counts, 
                                           'Error' = error_counts,
                                           'Accuracy Percentage' = accurate_counts/(accurate_counts+error_counts)))
}

confusion_matrix
```

## Conclusion

Accuracy is not that great, however reason behind this is most of our documents have equal mix of all 4 topics. Also, one of the topics "Donald Trump" has its words all over, and Brexit/ Trump/Bitcoin do have some relations

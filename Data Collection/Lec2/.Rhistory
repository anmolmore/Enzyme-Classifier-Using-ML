suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
movie.nodes = html_nodes(page,'.titleColumn a')
# Check one node
xmlTreeParse(movie.nodes[[1]])
require(magrittr)
movie.link = movie.nodes %>% html_attr("href")
movie.link[1:5]   # view a few
movie.link = paste("http://www.imdb.com", movie.link, sep="")  # prefixing siteURL
cat("\n")
cat("A sample of movie.link we scraped\n")
movie.link[1:5]   # view a few
cat("\n")
movie.cast = movie.nodes %>% html_attr("title")
cat("A sample of movie.cast we scraped\n")
movie.cast[1:5]   # view a few
cat("\n")
movie.name = movie.nodes %>% html_text()
cat("A sample of movie.name we scraped\n")
movie.name[1:4]
year = page %>% html_nodes(".secondaryInfo") %>% html_text()
year[1:4]
# post-process to get numeric years
year = year %>% gsub(")", "", .) %>%   # remove trailing ')'
gsub("\\(", "", .) %>%  # remove first '('
as.numeric()
year[1:5]
rating.nodes = page %>% html_nodes('.imdbRating')
# Check One node
xmlTreeParse(rating.nodes[[1]])
# Collect and correct the node
rating.nodes = page %>% html_nodes('.imdbRating strong')
votes = rating.nodes %>% html_attr('title')
votes[1:4]
# post-process & cleanup
votes = votes %>% gsub('.*?based on ','', .) %>%
gsub(' user ratings','', .) %>%
gsub(',', '', .)
votes[1:4]
rating = rating.nodes %>% html_text() %>% as.numeric()
rating[1:5]
top250 = data.frame(movie.name, movie.cast, movie.link,year,votes,rating)
head(top250)
write.csv(top250,'IMDB Top 250.csv', row.names = F) # writes to getwd()
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
View(page)
movie.nodes = html_nodes(page,'.titleColumn a')
View(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
# define tgt url
url <- 'https://glossary.atis.org/search-results/?char=A'
# extract url content with read_html() func
url_content <- read_html(url)
# alternately, above in 1 line.
# url_content <- read_html('http://www.atis.org/glossary/definitionsList.aspx?find=A&kw=0')
# alternately, using pipes operator %>%
url_content <- url %>% read_html()
## url.read_html()  // url%.%head_html
View(url_content)
# system.time({
# extracting all links in tgt page
links <-  url_content %>% html_nodes('a') %>%  # html_nodes() extracts ANY node type, as per argment
html_attr('href')    # html_attr() extracts particular info based on single node attribute
text <- url_content %>% html_nodes('a') %>% html_text()    # extracting ALL link text from tgt page
# }) # t = 0.84 secs
# creating a new dictionary of links and text extracted above
# Dealing with strings only, so stringsAsFactors as False
rough_dict <- data.frame(links, text, stringsAsFactors = F)
rough_dict[10:20,]
View(rough_dict)
fair_dict <- rough_dict %>%
filter(str_detect(links, 'glossary/.*/?char=')) %>%
select(text)
# Building a telecomindustry wordlist by scraping an alphabetical dict @ http://www.atis.org/glossary/
## Step 1: Load required libraries
library(rvest)
library(stringr)
library(dplyr)
library(magrittr)
# define tgt url
url <- 'https://glossary.atis.org/search-results/?char=A'
# extract url content with read_html() func
url_content <- read_html(url)
# alternately, above in 1 line.
# url_content <- read_html('http://www.atis.org/glossary/definitionsList.aspx?find=A&kw=0')
# alternately, using pipes operator %>%
url_content <- url %>% read_html()
## url.read_html()  // url%.%head_html
# system.time({
# extracting all links in tgt page
links <-  url_content %>% html_nodes('a') %>%  # html_nodes() extracts ANY node type, as per argment
html_attr('href')    # html_attr() extracts particular info based on single node attribute
text <- url_content %>% html_nodes('a') %>% html_text()    # extracting ALL link text from tgt page
# }) # t = 0.84 secs
# creating a new dictionary of links and text extracted above
# Dealing with strings only, so stringsAsFactors as False
rough_dict <- data.frame(links, text, stringsAsFactors = F)
rough_dict[10:20,]
fair_dict <- rough_dict %>%
filter(str_detect(links, 'glossary/.*/?char=')) %>%
select(text)
tail(fair_dict)
## Testing above workflow for one letter
# first construct url.
prefix_url = 'https://glossary.atis.org/search-results/?char='
test_url = paste0(prefix_url, "B"); test_url    # OK. But does it work?
# system.time({
# run workflow for newly constructed url
test_nodes <- test_url %>% read_html() %>% html_nodes('a')
test_links = test_nodes %>% html_attr('href')
test_text = test_nodes %>% html_text()
test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>%
filter(str_detect(test_links, 'glossary/.*/?char=')) %>%
select(test_text)
#  })    # t ~ 5 secs. It works.
# invoke func for each letter separately. More general that way than hard-coding A-Z also inside.
build_fair_dict <- function(test_url){
test_nodes <- test_url %>% read_html() %>% html_nodes('a')
test_links = test_nodes %>% html_attr('href')
test_text = test_nodes %>% html_text()
test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>%
filter(str_detect(test_links, 'glossary/.*/?char=')) %>%
select(test_text)
return(test_df)  } # func ends
## Looping over all the other letters
dict_content = vector(mode="list", length=length(LETTERS))    # list to store output
# system.time({
# commenting below coz of timeout issues I faced. Uncomment and run.
# for (i1 in 1:length(LETTERS)){     # type LETTERS in console and see
#   test_url = paste0(prefix_url, LETTERS[i1], suffix_url)
#   dict_content[[i1]] =  data.frame(LETTERS[i1], build_fair_dict(test_url))
#   cat(LETTERS[i1], " processed.", "\n")
#   }    # i1 loop ends
# })  # t = 127.13 secs
# Now bind list outp into df, check and close.
# suppressWarnings(dict_df = bind_rows(dict_content))
# str(dict_df)    # all good. :)
library(rvest)
content <- read_html('https://news.ycombinator.com')
## key is to identify the right css selector or xpath values of the html elements
# system.time({
title <- content %>% html_nodes('a.storylink') %>% # use CSS selector tool to ID node 'a.storylink'
html_text()   # within above, retain only text.
head(title, 10)
## More CSS selections made and stored in R objects
## Notes : .score was enough here - span.score gives the same result
score = content %>% html_nodes('span.score') %>% html_text()
age = content %>% html_nodes('span.age') %>% html_text()
## Stitch into a df now
df <- data.frame(title = title, score = score, age = age, stringsAsFactors=FALSE)
df %>% head(10)    # not working coz one link is missing when I tried it.
# just testing pulling out a full nodeset (tr.athing) instead of single node across nodesets.
athing_list = content %>% html_nodes('tr.athing')   # discovered 'athing' from page source as node-set element
length(athing_list)
# building empty vecs for colms
title_text = vector(mode="character", length=length(athing_list))
link_url = title_text    # so it has same list attribs as title_text.
source_site = title_text
# looping over each node-set
for (i1 in 1:length(athing_list)){
# some below may fail as nulls or NAs. Introducing html_attr() to get non-text content classes
a0 = athing_list[i1] %>% html_nodes('a.storylink') %>% html_text()
a1 = athing_list[i1] %>% html_nodes('a.storylink') %>% html_attr('href')
a2 = athing_list[i1] %>% html_nodes('span.sitestr') %>% html_text()
# seems empty vec outp was commonest error, so testing for the same using length()
ifelse(length(a0>0), {title_text[i1] = a0}, {title_text[i1] = "no title available"})
ifelse(length(a1>0), {link_url[i1] = a1}, {link_url[i1] = "no link available"})
ifelse(length(a2>0), {source_site[i1] = a2}, {source_site[i1] = "no site available"})
cat(i1, " processed.", "\n")
}    # i1 loop ends.  t = 0.78 secs
# now check if all lengths are same and then bind into a df
df = suppressWarnings(
data.frame(title_text, link_url, source_site, stringsAsFactors = FALSE))
head(df, 10)
# invoke func for each letter separately. More general that way than hard-coding A-Z also inside.
build_fair_dict <- function(test_url){
test_nodes <- test_url %>% read_html() %>% html_nodes('a')
test_links = test_nodes %>% html_attr('href')
test_text = test_nodes %>% html_text()
test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>%
filter(str_detect(test_links, 'glossary/.*/?char=')) %>%
select(test_text)
return(test_df)  } # func ends
## Looping over all the other letters
dict_content = vector(mode="list", length=length(LETTERS))    # list to store output
# system.time({
# commenting below coz of timeout issues I faced. Uncomment and run.
# for (i1 in 1:length(LETTERS)){     # type LETTERS in console and see
#   test_url = paste0(prefix_url, LETTERS[i1], suffix_url)
#   dict_content[[i1]] =  data.frame(LETTERS[i1], build_fair_dict(test_url))
#   cat(LETTERS[i1], " processed.", "\n")
#   }    # i1 loop ends
# })  # t = 127.13 secs
# Now bind list outp into df, check and close.
# suppressWarnings(dict_df = bind_rows(dict_content))
# str(dict_df)    # all good. :)
View(dict_content)
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top"
page = read_html(url)
movie.nodes = html_nodes(page,'.titleColumn a')
# Check one node
xmlTreeParse(movie.nodes[[1]])
View(movie.nodes)
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top"
page = read_html(url)
movie.nodes = html_nodes(page,'.titleColumn a')
# Check one node
xmlTreeParse(movie.nodes[[1]])
require(magrittr)
movie.link = movie.nodes %>% html_attr("href")
movie.link[1:5]   # view a few
movie.link = paste("http://www.imdb.com", movie.link, sep="")  # prefixing siteURL
cat("\n")
cat("A sample of movie.link we scraped\n")
movie.link[1:5]   # view a few
cat("\n")
movie.cast = movie.nodes %>% html_attr("title")
cat("A sample of movie.cast we scraped\n")
movie.cast[1:5]   # view a few
cat("\n")
movie.name = movie.nodes %>% html_text()
cat("A sample of movie.name we scraped\n")
movie.name[1:4]
year = page %>% html_nodes(".secondaryInfo") %>% html_text()
year[1:4]
# post-process to get numeric years
year = year %>% gsub(")", "", .) %>%   # remove trailing ')'
gsub("\\(", "", .) %>%  # remove first '('
as.numeric()
year[1:5]
rating.nodes = page %>% html_nodes('.imdbRating')
# Check One node
xmlTreeParse(rating.nodes[[1]])
# Collect and correct the node
rating.nodes = page %>% html_nodes('.imdbRating strong')
votes = rating.nodes %>% html_attr('title')
votes[1:4]
# post-process & cleanup
votes = votes %>% gsub('.*?based on ','', .) %>%
gsub(' user ratings','', .) %>%
gsub(',', '', .)
votes[1:4]
rating = rating.nodes %>% html_text() %>% as.numeric()
rating[1:5]
top250 = data.frame(movie.name, movie.cast, movie.link,year,votes,rating)
head(top250)
write.csv(top250,'IMDB Top 250.csv', row.names = F) # writes to getwd()

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])\n",
    "total_df = spark.createDataFrame([], schema)\n",
    "for file_name in os.listdir(\"/home/cloudera/data\"):\n",
    "    df = spark.read.option(\"header\", \"true\").text('file:///home/cloudera/data/' + file_name)\n",
    "    total_df = total_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The dataset is messy. Let us clean it up\n",
    "import re\n",
    "\n",
    "def process_line(x):\n",
    "    line = x['value']\n",
    "    parts = re.split(\"\\s+\",line,1)\n",
    "    sub_parts = re.split('--', parts[0])\n",
    "    parts_1 = ''\n",
    "    if len(sub_parts) > 1:\n",
    "       parts_1 = sub_parts[1] + ' ' + parts[1]\n",
    "    else:\n",
    "       parts_1 = parts[1]\n",
    "    return ([sub_parts[0],parts_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_rdd = total_df.rdd .filter(lambda x : x['value'] not in ['### introduction ###','### abstract ###']) .map(lambda x : process_line(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_df = input_rdd.toDF()\n",
    "input_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_df.groupBy('_1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case you need to do preprocessing of data\n",
    "#import gensim.parsing.preprocessing as gsp\n",
    "#from pyspark.sql.functions import udf\n",
    "#from pyspark.sql.types import StringType\n",
    "#from gensim import utils\n",
    "\n",
    "\n",
    "#filters = [\n",
    "#           gsp.strip_tags, \n",
    "#           gsp.strip_punctuation,\n",
    "#           gsp.strip_multiple_whitespaces,\n",
    "#           gsp.strip_numeric,\n",
    "#           gsp.remove_stopwords, \n",
    "#           gsp.strip_short, \n",
    "#           gsp.stem_text\n",
    "#          ]\n",
    "\n",
    "#def clean_text(x):\n",
    "#    s = x[1]\n",
    "#    s = s.lower()\n",
    "#    s = utils.to_unicode(s)\n",
    "#    for f in filters:\n",
    "#        s = f(s)\n",
    "#    return (x[0],s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"_2\", outputCol=\"tokens\")\n",
    "w2v = Word2Vec(vectorSize=300, minCount=0, inputCol=\"tokens\", outputCol=\"features\")\n",
    "doc2vec_pipeline = Pipeline(stages=[tokenizer,w2v])\n",
    "doc2vec_model = doc2vec_pipeline.fit(input_df)\n",
    "doc2vecs_df = doc2vec_model.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vecs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_train_df, w2v_test_df = doc2vecs_df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "si = StringIndexer(inputCol=\"_1\", outputCol=\"label\")\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "rf_classifier_pipeline = Pipeline(stages=[si,rf_classifier])\n",
    "rf_predictions = rf_classifier_pipeline.fit(w2v_train_df).transform(w2v_test_df)\n",
    "\n",
    "rf_model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = rf_model_evaluator.evaluate(rf_predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression(family=\"multinomial\")\n",
    "\n",
    "lr_classifier_pipeline = Pipeline(stages=[si,lr_classifier])\n",
    "lr_predictions = lr_classifier_pipeline.fit(w2v_train_df).transform(w2v_test_df)\n",
    "\n",
    "lr_model_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = lr_model_evaluator.evaluate(lr_predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda2]",
   "language": "python",
   "name": "Python [anaconda2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

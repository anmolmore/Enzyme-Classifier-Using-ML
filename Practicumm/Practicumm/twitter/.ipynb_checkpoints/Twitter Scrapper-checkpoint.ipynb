{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x10a8704a8>\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"eQ8ErZL3vEvZkQvTSKMPndhzb\"\n",
    "consumer_secret = \"ztZJo14QU1wFtREneeQ7HhBUbK2RidqIIHCvXKSZDjEqVMTprv\"\n",
    "\n",
    "access_token = \"115683208-cdY9Yru9aaStNHr9QtA2uLTn4khAqpoL33HvPKgm\"\n",
    "access_token_secret = \"4vheUcufR5nPw9WAYbMs0omnjc97KmYaN2fk3dYQ0O1UO\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_tweet(tweet) :\n",
    "    tweet_date = tweet['created_at']\n",
    "    ts = time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet_date,'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "\n",
    "    tweet_id = tweet['id']\n",
    "    tweet_text = tweet['text']\n",
    "    if('extended_tweet' in tweet.keys()):\n",
    "        tweet_text = tweet['extended_tweet']['full_text']\n",
    "\n",
    "    user_id = tweet['user']['id']\n",
    "    followers_count = tweet['user']['followers_count']\n",
    "    friends_count = tweet['user']['friends_count']\n",
    "\n",
    "    user_mentions = tweet['entities']['user_mentions']\n",
    "    screen_names = [user_mention['screen_name'] for user_mention in user_mentions]\n",
    "    screen_name = tweet['user']['screen_name']\n",
    "    retweet_count = tweet['retweet_count']\n",
    "    favorite_count = tweet['favorite_count']\n",
    "    tweet_row = {'date':ts,\n",
    "                 'tweet_id' : tweet_id,\n",
    "                 'user_id' : user_id,\n",
    "                 'followers_count' : followers_count,\n",
    "                 'friends_count' : friends_count,\n",
    "                 'user_mentions' : screen_names,\n",
    "                 'screen_name' : screen_name,\n",
    "                 'retweet_count' : retweet_count,\n",
    "                 'favorite_count' : favorite_count,\n",
    "                 'full_text':tweet_text}\n",
    "    return tweet_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref : https://stackoverflow.com/questions/1060279/iterating-through-a-range-of-dates-in-python\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201906020000\n",
      "201906022359\n",
      "{}\n",
      "web_request_count:  1\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJjODY5Y2YwMjYwMzJhOWJlZGNlOTE4MWJmNzhjNTNjMDQ4ZGZkOWFjYTg1OGI5ZmQ4ZDhkZWM1ZmFkNzhjN2U3IiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTI1MTY1NDk3MjkyMzkwNC0wIn0=\n",
      "web_request_count:  2\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiI4YTAwZjVlM2Y5ZDlhNzNhNDc5MWVhN2U5NTIzMTIwNDJiMWQxMDJkZTA5YTNiMTA2MjE4NjJkZTk3OGQ5NWM5IiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTIzODQyNDE1NDI1NTM2Mi0wIn0=\n",
      "web_request_count:  3\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiI0MDk5ZTU4Yjk0YWVmZDk2ZTFmZWY0MDUzMmEwZDM1MmIxZWMzZTA2ZGZlYzBjMzdkZGViOTMzNWQwZDg2YmVjIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTIyNDAzMzAzNTY4NTg4OC0wIn0=\n",
      "web_request_count:  4\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiIyMGE5MDc3NTQ4ZTJjYWVlMzRjOGZjY2ZjNDYxZDMwOWI4NjRiNWEzNjdmMWVmNTUzNDkwYzcyOTc0ZjdhZjZhIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTIxMTY2Mzc5NDIyMTA1Ni0wIn0=\n",
      "web_request_count:  5\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJjYmI0NzM1ZWI3NWJmYzg0N2FiOWIwMzlhMTUyNTU5MzU0OWU3ZjVhYjU3NmE5ZGRlNWQ1ZTMxOWM4YzU2MTQ4IiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTE4NjQ1ODgwODAxNjg5OC0wIn0=\n",
      "web_request_count:  6\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiI4NDBjMjdjYmNjZWE1YjgzOTljZDYxMDUwMDEzNGJhZjFjMjRlMmI5ZTVkYzQyZTUxZjlmOGNmNTlmNGY2OTFjIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTE1MDAxOTkwNjkyMDQ0OC0wIn0=\n",
      "web_request_count:  7\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiIwNzQ5N2ZiOTRiNGRlNTJiN2E5YTE5MmY1Y2UzYjM1MDY3ZTRkZTJhY2I2MGJlODhmOTM4OTRmOGExNmVjMzUwIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTEzNDQ0MTk1MTUyMjgxNi0wIn0=\n",
      "web_request_count:  8\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJmZWNiZTRkYjAzM2MxNTgyMTdjOWNjN2JhNTEzODM1NzI3MDE1ZmZkZmE0NjIxMjVhMmI0Zjk5MDJlMWE0ZWEwIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTExMjg1NDE0MTQwNzIzMy0wIn0=\n",
      "web_request_count:  9\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJjN2UxZDUwNjE4NDFjYWU0OTUxNTRkOWI5NzcwYWZkYWIzM2FjNDA2Yjk3MDJjMTYyZmQ1OGRmMDQxMWQ5OTJjIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTA5NTA0MzMyMzA5NzA4OC0wIn0=\n",
      "web_request_count:  10\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJmNGVkNDI4NzA2OTVjZDhmN2RmNGI0MzYxNzJhZGJjNGFlM2RhMGZhN2VlNTNlNGE4NmIyY2MyOWU4MDk2ODkxIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTA2NjgzOTMwNDc1NzI0OC0wIn0=\n",
      "web_request_count:  11\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiI2NjQxZDA5NDgwMjQ1NjBlMGMyM2MyZDM2ZDk0MjYzZDljY2E2ODcxYmY3ZDUwNDdmZjljNTk4NmE4ZjQ2OGYwIiwiZnJvbURhdGUiOiIyMDE5MDYwMjAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMjIzNTkiLCJuZXh0IjoiMjAxOTA2MDIyMzU5MDAtMTEzNTAzMjY1MzY2MzYxNzAyNC0wIn0=\n",
      "web_request_count:  12\n",
      "status_code:  200\n",
      "(1149, 10)\n",
      "201906030000\n",
      "201906032359\n",
      "{}\n",
      "web_request_count:  13\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJlYTY5MTk4NTk5MjY3ZDM5ZDA4NzQwZGYwZjRmN2ViMGMzZjNkNzVjZTcwNDFlMzRhYzlkZWQwZmYwNmUyMWFjIiwiZnJvbURhdGUiOiIyMDE5MDYwMzAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMzIzNTkiLCJuZXh0IjoiMjAxOTA2MDMyMzU5MDAtMTEzNTYwNTA4ODQwNjM3NjQ0OC0wIn0=\n",
      "web_request_count:  14\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJhN2UzMTUwYzQyMzgwYzYyMTA1MTZiNDkzZDlhODliOTc2YzRhMmMzYTVjOWRkMDg0YzFkNTM3OWM5ZjQ2MmY0IiwiZnJvbURhdGUiOiIyMDE5MDYwMzAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMzIzNTkiLCJuZXh0IjoiMjAxOTA2MDMyMzU5MDAtMTEzNTU3Njk0NzE0NzE5ODQ2NC0wIn0=\n",
      "web_request_count:  15\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiI3ZWUzYjhhY2MzYTAwODk1MDg3Y2MyNzJlZmY0MDljMTJlNmIyMjZiNTE0ZjY0NzlhM2RhMmYyYzdlZjJiMWMxIiwiZnJvbURhdGUiOiIyMDE5MDYwMzAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMzIzNTkiLCJuZXh0IjoiMjAxOTA2MDMyMzU5MDAtMTEzNTU1NDc0NjkzOTQwNDI4OC0wIn0=\n",
      "web_request_count:  16\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJkOTc3MDM2NGY1ZDg3MTU0OTg3NTIyMjMxZDA0ZjMwNzYxNzYzNTUzODFlZjkwN2ZmM2ViYTgzYjU4OTdhNjk4IiwiZnJvbURhdGUiOiIyMDE5MDYwMzAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMzIzNTkiLCJuZXh0IjoiMjAxOTA2MDMyMzU5MDAtMTEzNTUxODUzNDAwMjMzMTY1MC0wIn0=\n",
      "web_request_count:  17\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiJhYWQwZTk4YTRhZTI2MDU1NDFjZGU1MmE0MzhmNWM1ZjM1MjQ0OTY4OGFjYzBjMzc5YjFlMzFiMTVmNmU4MDdhIiwiZnJvbURhdGUiOiIyMDE5MDYwMzAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMzIzNTkiLCJuZXh0IjoiMjAxOTA2MDMyMzU5MDAtMTEzNTQ5MjYxMjgxMzU3NDE0NS0wIn0=\n",
      "web_request_count:  18\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiIzOWM1ZTA0MWIxZTc2YzViMzk0YTJjYjMwNDlkMTFiNjZlZTNhMmRiZDg3ODFiMmVlMjBmMDYwMGQyMDhiYTVlIiwiZnJvbURhdGUiOiIyMDE5MDYwMzAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMzIzNTkiLCJuZXh0IjoiMjAxOTA2MDMyMzU5MDAtMTEzNTQ0OTM4MTA1MjY0MTI4MC0wIn0=\n",
      "web_request_count:  19\n",
      "status_code:  200\n",
      "eyJhdXRoZW50aWNpdHkiOiI5ZTkyODUzMTRkZjVhOTI3ZDNhYmNkMWRiOGIxMDhjNjIzYTVlODI4ZTI1NTQwNWQ2MTczZmUzOTBiMzcxNjA4IiwiZnJvbURhdGUiOiIyMDE5MDYwMzAwMDAiLCJ0b0RhdGUiOiIyMDE5MDYwMzIzNTkiLCJuZXh0IjoiMjAxOTA2MDMyMzU5MDAtMTEzNTM5NzY4MTI0NzgzODIwOC0wIn0=\n",
      "web_request_count:  20\n",
      "status_code:  200\n",
      "(766, 10)\n"
     ]
    }
   ],
   "source": [
    "#Ref : https://medium.com/@poconnell732/acquiring-free-historical-geo-located-data-from-twitter-1f8f2821e9b1\n",
    "from TwitterAPI import TwitterAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "\n",
    "api = TwitterAPI(consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token_key=access_token,\n",
    "                 access_token_secret=access_token_secret)\n",
    "PRODUCT = '30day' #Using 30day API\n",
    "LABEL = 'devbox2' # sandbox name\n",
    "web_request_count = 0\n",
    "                \n",
    "start_month_date = date(2019, 6, 4)\n",
    "end_month_date = date(2019, 6, 5)\n",
    "\n",
    "for date in daterange(start_month_date, end_month_date):\n",
    "    list_tweets = []\n",
    "    next_token = {}\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    start_date = date_str + \"0000\"\n",
    "    end_date = date_str + \"2359\"\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    \n",
    "    while next_token is not None :\n",
    "        print(next_token)\n",
    "        if not next_token :\n",
    "            req_dict = {'query' : '@swiggy_in OR @swiggyCares lang:en', 'fromDate': start_date, 'toDate': end_date}\n",
    "        else :\n",
    "            req_dict['next'] = next_token\n",
    "        \n",
    "        web_request_count += 1\n",
    "        print('web_request_count: ', web_request_count)\n",
    "        req = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), req_dict)\n",
    "        print('status_code: ', req.status_code)\n",
    "        response = req.json()\n",
    "        if('next' in response.keys()):\n",
    "            next_token = response['next']\n",
    "        else :\n",
    "            next_token = None\n",
    "        \n",
    "        results = response['results']\n",
    "        with open('swiggy/json/' + date_str + '_' + str(web_request_count) + '.json', 'w+') as f:\n",
    "            json.dump(results, f)\n",
    "        \n",
    "        #print(results)\n",
    "        for tweet in results:\n",
    "            tweet_row = process_tweet(tweet)\n",
    "            list_tweets.append(tweet_row)\n",
    "        df = pd.DataFrame(list_tweets)\n",
    "    df.to_csv('swiggy/' + date_str + '.csv', index=False)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref : https://medium.com/@poconnell732/acquiring-free-historical-geo-located-data-from-twitter-1f8f2821e9b1\n",
    "from TwitterAPI import TwitterAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "\n",
    "api = TwitterAPI(consumer_key=consumer_key,\n",
    "                 consumer_secret=consumer_secret,\n",
    "                 access_token_key=access_token,\n",
    "                 access_token_secret=access_token_secret)\n",
    "PRODUCT = '30day' #Using 30day API\n",
    "LABEL = 'devbox2' # sandbox name\n",
    "web_request_count = 0\n",
    "                \n",
    "start_month_date = date(2019, 6, 4)\n",
    "end_month_date = date(2019, 6, 5)\n",
    "\n",
    "for date in daterange(start_month_date, end_month_date):\n",
    "    list_tweets = []\n",
    "    next_token = {}\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    start_date = date_str + \"0000\"\n",
    "    end_date = date_str + \"2359\"\n",
    "    print(start_date)\n",
    "    print(end_date)\n",
    "    \n",
    "    while next_token is not None :\n",
    "        print(next_token)\n",
    "        if not next_token :\n",
    "            req_dict = {'query' : '@zomatocare OR @zomatoIn lang:en', 'fromDate': start_date, 'toDate': end_date}\n",
    "        else :\n",
    "            req_dict['next'] = next_token\n",
    "        \n",
    "        web_request_count += 1\n",
    "        print('web_request_count: ', web_request_count)\n",
    "        req = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), req_dict)\n",
    "        print('status_code: ', req.status_code)\n",
    "        response = req.json()\n",
    "        if('next' in response.keys()):\n",
    "            next_token = response['next']\n",
    "        else :\n",
    "            next_token = None\n",
    "        \n",
    "        results = response['results']\n",
    "        with open('zomato/json/' + date_str + '_' + str(web_request_count) + '.json', 'w+') as f:\n",
    "            json.dump(results, f)\n",
    "        \n",
    "        #print(results)\n",
    "        for tweet in results:\n",
    "            tweet_row = process_tweet(tweet)\n",
    "            list_tweets.append(tweet_row)\n",
    "        df = pd.DataFrame(list_tweets)\n",
    "    df.to_csv('zomato/' + date_str + '.csv', index=False)\n",
    "    print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

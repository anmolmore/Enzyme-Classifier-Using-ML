---
title: "Webscraping basics with R"
output: html_document
---
## Ref - https://datascienceplus.com/building-a-telecom-dictionary-scraping-web-using-rvest-in-r/
## R's rvest for webscraping: some examples

Early era work in text-An relied a lot on wordlists, often built manually.
Let's scrape a glossary (of telecom terms) and build a wordlist using the same in the first of today's simple exercises.

### Step 1: Load Required libraries

```{r}
# Building a telecomindustry wordlist by scraping an alphabetical dict @ http://www.atis.org/glossary/
## Step 1: Load required libraries
library(rvest)
library(stringr)
library(dplyr)
library(magrittr)
```

First off, go examine and explore the website itself. http://www.atis.org/glossary/
Examine the URL structure. Seems like only the letters 'A', 'B' etc need to change.

### Step 2: Code to Pull requisite Data for One Unit (Letter)

Plan is simple - we'll figure out how to write rvest code to pull data out for one letter, then rinse and repeat for the rest of the letters.

```{r}
# define tgt url
url <- 'https://glossary.atis.org/search-results/?char=A'    

# extract url content with read_html() func
url_content <- read_html(url)    

# alternately, above in 1 line.
# url_content <- read_html('http://www.atis.org/glossary/definitionsList.aspx?find=A&kw=0')

# alternately, using pipes operator %>%
url_content <- url %>% read_html()    
```

Now introducing html_nodes(), html_attr() and html_text() funcs in rvest library.

How to know which HTML tags are useful? View right_click + 'page source' and check HTML page structure yourself first.

A better alternative is using the CSS selector gadget supported in the Google Chrome Browser.

```{r}
# system.time({

 # extracting all links in tgt page 
 links <-  url_content %>% html_nodes('a') %>%  # html_nodes() extracts ANY node type, as per argment
              html_attr('href')    # html_attr() extracts particular info based on single node attribute

 text <- url_content %>% html_nodes('a') %>% html_text()    # extracting ALL link text from tgt page

# }) # t = 0.84 secs

# creating a new dictionary of links and text extracted above
# Dealing with strings only, so stringsAsFactors as False
rough_dict <- data.frame(links, text, stringsAsFactors = F)
rough_dict[10:20,]

```


Well, from the above, seems like we'll need to filter the glossary terms to drop irrelevant information.

Examine the structure carefully and note that the glossary terms contain the pattern 'glossary/.*/?char='. 

Let's make use of this pattern to select the relevant portion first.

```{r}
fair_dict <- rough_dict %>% 
        filter(str_detect(links, 'glossary/.*/?char=')) %>% 
        select(text)

 tail(fair_dict)
```
The above is much better, isn't it? So why not now put together all that we did above into a single piece of code?

```{r}
## Testing above workflow for one letter
 # first construct url.
 prefix_url = 'https://glossary.atis.org/search-results/?char='
 test_url = paste0(prefix_url, "B"); test_url    # OK. But does it work?
```

```{r}
# system.time({ 

 # run workflow for newly constructed url
 test_nodes <- test_url %>% read_html() %>% html_nodes('a')
 test_links = test_nodes %>% html_attr('href')
 test_text = test_nodes %>% html_text()
 test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>% 
        filter(str_detect(test_links, 'glossary/.*/?char=')) %>% 
        select(test_text)

#  })    # t ~ 5 secs. It works. 
```


### Step 3: Write a small func to pull data for one letter

Time now to encode the above workflow into one func, which we'll repeatedly invoke by looping across letters.

```{r}
# invoke func for each letter separately. More general that way than hard-coding A-Z also inside.
build_fair_dict <- function(test_url){    

 test_nodes <- test_url %>% read_html() %>% html_nodes('a')
 test_links = test_nodes %>% html_attr('href')
 test_text = test_nodes %>% html_text()
 test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>% 
        filter(str_detect(test_links, 'glossary/.*/?char=')) %>% 
        select(test_text) 
 
 return(test_df)  } # func ends

## Looping over all the other letters
dict_content = vector(mode="list", length=length(LETTERS))    # list to store output

# system.time({
# commenting below coz of timeout issues I faced. Uncomment and run.
# for (i1 in 1:length(LETTERS)){     # type LETTERS in console and see

#   test_url = paste0(prefix_url, LETTERS[i1], suffix_url) 
#   dict_content[[i1]] =  data.frame(LETTERS[i1], build_fair_dict(test_url))
#   cat(LETTERS[i1], " processed.", "\n")
    
#   }    # i1 loop ends

# })  # t = 127.13 secs

# Now bind list outp into df, check and close.
# suppressWarnings(dict_df = bind_rows(dict_content))
# str(dict_df)    # all good. :)

```

That's it. We're done with a simple rvest web scraping exercise.

Clearly, we can scrape much more. For instance, the link behind each glossary term and so on. We'll get there in good time.

Quick-check: So, what all funcs do you remember we used? List the same.

## Another example. Scraping a hacker website.

Cool, eh?

I'll be more consice here. See if you can follow the steps.

```{r}
library(rvest)
content <- read_html('https://news.ycombinator.com')

## key is to identify the right css selector or xpath values of the html elements

# system.time({

 title <- content %>% html_nodes('a.storylink') %>% # use CSS selector tool to ID node 'a.storylink'
            html_text()   # within above, retain only text.
 head(title, 10)
```

```{r}
## More CSS selections made and stored in R objects
## Notes : .score was enough here - span.score gives the same result 
 score = content %>% html_nodes('span.score') %>% html_text()
 age = content %>% html_nodes('span.age') %>% html_text()

## Stitch into a df now
 df <- data.frame(title = title, score = score, age = age, stringsAsFactors=FALSE)
 df %>% head(10)    # not working coz one link is missing when I tried it.
```

Above didn't work for when I tried it coz one of the links didn;t have a source site.

When such happens, R will simply proceed to the next item (sometimes, it may even stop execution with error).

What that does is mess up the data frame formatting and sequence. Is there a better way? Sure there is. See below.

### Handling exceptions like missing vals.

One logical way to make this work would be to redefine each 'unit' that we parse and analyze.

So, let's define and pull out the entire nodeset for each link at one go, rather than pickup similar nodes from the entire doc at one go.

```{r}
# just testing pulling out a full nodeset (tr.athing) instead of single node across nodesets.
 athing_list = content %>% html_nodes('tr.athing')   # discovered 'athing' from page source as node-set element
 length(athing_list)
```

```{r}
# building empty vecs for colms
 title_text = vector(mode="character", length=length(athing_list))
 link_url = title_text    # so it has same list attribs as title_text.
 source_site = title_text

# looping over each node-set
 for (i1 in 1:length(athing_list)){

 # some below may fail as nulls or NAs. Introducing html_attr() to get non-text content classes
 a0 = athing_list[i1] %>% html_nodes('a.storylink') %>% html_text()
 a1 = athing_list[i1] %>% html_nodes('a.storylink') %>% html_attr('href')    
 a2 = athing_list[i1] %>% html_nodes('span.sitestr') %>% html_text()

 # seems empty vec outp was commonest error, so testing for the same using length()
 ifelse(length(a0>0), {title_text[i1] = a0}, {title_text[i1] = "no title available"}) 
 ifelse(length(a1>0), {link_url[i1] = a1}, {link_url[i1] = "no link available"}) 
 ifelse(length(a2>0), {source_site[i1] = a2}, {source_site[i1] = "no site available"})

 cat(i1, " processed.", "\n")
 
 }    # i1 loop ends.  t = 0.78 secs
```

Notice what we did there? We defined each analysis element as the .athing nodeset

```{r}
# now check if all lengths are same and then bind into a df
df = suppressWarnings(
  data.frame(title_text, link_url, source_site, stringsAsFactors = FALSE))
head(df, 10)
```

Same result as last time, just the way to get there was different.

Q: So when might you prefer the latter approach and when the former?

That's it for now. We'll see more about using python for web scraping later.

Sudhir
head(xv,10)
#We subtract 108 from each of numbers above, and get absolute values
x <- abs(xv-108)
head(x,10)
#We try to get min of all absolute values we got above
y <- min(abs(xv-108))
y
#So the value in xv which becomes close of min(abs(xv-108)) will be the number closest to 108 in this distribution
#This statement gives index at which that number is present
which(abs(xv-108)==min(abs(xv-108)))
#Reference - https://www.oreilly.com/library/view/the-r-book/9780470510247/ch002-sec020.html
#And the luckiest closest number is ..
closest<-function(xv,sv) {
xv[which(abs(xv-sv)==min(abs(xv-sv)))]
}
closest(xv,108)
#This generates 1000 random numbers
xv <- rnorm(1000,100,10)
#For easy understanding let's do same thing with 10 random numbers with mean=100, and sd=10.
#Can I say, most of numbers will revolve between 90-100-110
#can be rewritten as below
xv <- rnorm(10,mean=100,sd=10)
head(xv,10)
#We subtract 108 from each of numbers above, and get absolute values
x <- abs(xv-108)
head(x,10)
#We try to get min of all absolute values we got above
y <- min(abs(xv-108))
y
#So the value in xv which becomes close of min(abs(xv-108)) will be the number closest to 108 in this distribution
#This statement gives index at which that number is present
which(abs(xv-108)==min(abs(xv-108)))
#Reference - https://www.oreilly.com/library/view/the-r-book/9780470510247/ch002-sec020.html
#And the luckiest closest number is ..
closest<-function(xv,sv) {
xv[which(abs(xv-sv)==min(abs(xv-sv)))]
}
closest(xv,108)
#This generates 1000 random numbers
xv <- rnorm(1000,100,10)
#For easy understanding let's do same thing with 10 random numbers with mean=100, and sd=10.
#Can I say, most of numbers will revolve between 90-100-110
#can be rewritten as below
xv <- rnorm(10,mean=100,sd=10)
head(xv,10)
#We subtract 108 from each of numbers above, and get absolute values
x <- abs(xv-108)
head(x,10)
#We try to get min of all absolute values we got above
y <- min(abs(xv-108))
y
#So the value in xv which becomes close of min(abs(xv-108)) will be the number closest to 108 in this distribution
#This statement gives index at which that number is present
which(abs(xv-108)==min(abs(xv-108)))
#Reference - https://www.oreilly.com/library/view/the-r-book/9780470510247/ch002-sec020.html
#And the luckiest closest number is ..
closest<-function(xv,sv) {
xv[which(abs(xv-sv)==min(abs(xv-sv)))]
}
closest(xv,108)
squrt<-function(x){
X<-sqrt(x)
Ifelse(x>0,print(x),"Negative No Provided")
}
squrt(-16)
Ifelse(x>0,print(x),return("Negative No Provided"))
return result
squrt<-function(x){
X<-sqrt(x)
result <- Ifelse(x>0,print(x),"Negative No Provided")
return result
}
squrt<-function(x){
X <- sqrt(x)
result <- Ifelse(x > 0, X,"Negative No Provided")
return result
}
result <- ifelse(x > 0, X,"Negative No Provided")
X <- sqrt(x)
result <- ifelse(x > 0, X,"Negative No Provided")
return result
squrt<-function(x) {
X <- sqrt(x)
result <- ifelse(x > 0, X,"Negative No Provided")
return result
}
squrt<-function(x){
X<-sqrt(x)
Ifelse(x>0,print(x),"Negative No Provided")
}
squrt(-16)
ifelse(x>0,print(x),"Negative No Provided")
squrt<-function(x){
X<-sqrt(x)
ifelse(x>0,print(x),"Negative No Provided")
}
squrt(-16)
squrt<-function(x){
X<-sqrt(x)
return(ifelse(x>0,print(x),"Negative No Provided"))
}
squrt(-16)
squrt<-function(x){
return(ifelse(x>0,sqrt(x),"Negative No Provided"))
}
squrt(-16)
ifelse(x>0,sqrt(x),"Negative No Provided")
squrt<-function(x){
ifelse(x>0,sqrt(x),"Negative No Provided")
}
squrt(-16)
squrt<-function(x){
X<-sqrt(x)
ifelse(x>0,print(x),"Negative No Provided")
}
squrt(16)
squrt<-function(x){
X<-sqrt(x)
Ifelse(x>0,print(x),"Negative No Provided")
}
X<-sqrt(x)
squrt<-function(x){
X<-sqrt(x)
Ifelse(x>0,print(x),"Negative No Provided")
}
squrt(-16)
squrt<-function(x){
X<-sqrt(x)
ifelse(x>0,print(x),"Negative No Provided")
}
squrt(-16)
squrt(16)
sqrt(-16)
squrt<-function(x){
X<-sqrt(x)
ifelse(x>0,print(x),"Negative No Provided")
}
squrt(-16)
squrt<-function(x){
X<-sqrt(x)
ifelse(x>0,x,"Negative No Provided")
}
squrt(-16)
squrt<-function(x){
X<-sqrt(x)
ifelse(x>0,X,"Negative No Provided")
}
squrt(-16)
squrt<-function(xy){
X<-sqrt(xy)
ifelse(xy>0,X,"Negative No Provided")
}
squrt(-16)
X<-sqrt(-16)
ifelse(-16>0,X,"Negative No Provided")
squrt<-function(xy){
X<-sqrt(xy)
ifelse(xy>0,X,"Negative No Provided")
}
squrt(-16)
# JSM App
source("https://raw.githubusercontent.com/sudhir-voleti/jsm-shinyapp/master/dependency-jsm-shinyapp.R")
install.packages("shiny")
install.packages("fmsb")
runGitHub("jsm-shinyapp","sudhir-voleti")
# JSM App
try(require("shiny")||install.packages("shiny"))
try(require("fmsb")||install.packages("fmsb"))
library("shiny")
library("fmsb")
runGitHub("jsm-shinyapp","sudhir-voleti")
runGitHub("jsm-shinyapp","sudhir-voleti")
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
install.packages("rvest")
install.packages("XML")
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
movie.nodes = html_nodes(page,'.titleColumn a')
# Check one node
xmlTreeParse(movie.nodes[[1]])
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
install.packages("magrittr")
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
head(movie.nodes)
movie.nodes = html_nodes(page,'.titleColumn')
head(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
movie.nodes = html_nodes(page,'.titleColumn a')
head(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
movie.nodes = html_nodes(page,'.titleColumn')
head(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
movie.nodes = html_nodes(page,'.titleColumn, a')
head(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
movie.nodes = html_nodes(page,'.titleColumn a')
head(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
movie.nodes = html_nodes(page,'.titleColumn a')
#head(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
require(magrittr)
movie.link = movie.nodes %>% html_attr("href")
movie.link[1:5]   # view a few
movie.link = paste("http://www.imdb.com", movie.link, sep="")  # prefixing siteURL
cat("\n")
cat("A sample of movie.link we scraped\n")
movie.link[1:5]   # view a few
cat("\n")
movie.cast = movie.nodes %>% html_attr("title")
cat("A sample of movie.cast we scraped\n")
movie.cast[1:5]   # view a few
cat("\n")
movie.name = movie.nodes %>% html_text()
cat("A sample of movie.name we scraped\n")
movie.name[1:4]
install.packages("read_xml")
suppressPackageStartupMessages({
if (!require(rvest)){install.packages("rvest")}
if (!require(XML)){install.packages("XML")}
library("rvest")
library("XML")
})
# IMDB Top 250 Movies
url = "http://www.imdb.com/chart/top?ref_=nv_wl_img_3"
page = read_html(url)
movie.nodes = html_nodes(page,'.titleColumn a')
#head(movie.nodes)
# Check one node
xmlTreeParse(movie.nodes[[1]])
require(magrittr)
movie.link = movie.nodes %>% html_attr("href")
movie.link[1:5]   # view a few
movie.link = paste("http://www.imdb.com", movie.link, sep="")  # prefixing siteURL
cat("\n")
cat("A sample of movie.link we scraped\n")
movie.link[1:5]   # view a few
cat("\n")
movie.cast = movie.nodes %>% html_attr("title")
cat("A sample of movie.cast we scraped\n")
movie.cast[1:5]   # view a few
cat("\n")
movie.name = movie.nodes %>% html_text()
cat("A sample of movie.name we scraped\n")
movie.name[1:4]
year = page %>% html_nodes(".secondaryInfo") %>% html_text()
year[1:4]
# post-process to get numeric years
year = year %>% gsub(")", "", .) %>%   # remove trailing ')'
gsub("\\(", "", .) %>%  # remove first '('
as.numeric()
year[1:5]
rating.nodes = page %>% html_nodes('.imdbRating')
# Check One node
xmlTreeParse(rating.nodes[[1]])
# Collect and correct the node
rating.nodes = page %>% html_nodes('.imdbRating strong')
votes = rating.nodes %>% html_attr('title')
votes[1:4]
# post-process & cleanup
votes = votes %>% gsub('.*?based on ','', .) %>%
gsub(' user ratings','', .) %>%
gsub(',', '', .)
votes[1:4]
rating = rating.nodes %>% html_text() %>% as.numeric()
rating[1:5]
top250 = data.frame(movie.name, movie.cast, movie.link,year,votes,rating)
head(top250)
write.csv(top250,'IMDB Top 250.csv', row.names = F) # writes to getwd()
# Building a telecomindustry wordlist by scraping an alphabetical dict @ http://www.atis.org/glossary/
## Step 1: Load required libraries
library(rvest)
library(stringr)
library(dplyr)
library(magrittr)
# define tgt url
url <- 'https://glossary.atis.org/search-results/?char=A'
# extract url content with read_html() func
url_content <- read_html(url)
# alternately, above in 1 line.
# url_content <- read_html('http://www.atis.org/glossary/definitionsList.aspx?find=A&kw=0')
# alternately, using pipes operator %>%
url_content <- url %>% read_html()
# system.time({
# extracting all links in tgt page
links <-  url_content %>% html_nodes('a') %>%  # html_nodes() extracts ANY node type, as per argment
html_attr('href')    # html_attr() extracts particular info based on single node attribute
text <- url_content %>% html_nodes('a') %>% html_text()    # extracting ALL link text from tgt page
# }) # t = 0.84 secs
# creating a new dictionary of links and text extracted above
rough_dict <- data.frame(links, text, stringsAsFactors = F)
rough_dict[10:20,]
fair_dict <- rough_dict %>%
filter(str_detect(links, 'glossary/.*/?char=')) %>%
select(text)
tail(fair_dict)
## Testing above workflow for one letter
# first construct url.
prefix_url = 'https://glossary.atis.org/search-results/?char='
test_url = paste0(prefix_url, "B"); test_url    # OK. But does it work?
# system.time({
# run workflow for newly constructed url
test_nodes <- test_url %>% read_html() %>% html_nodes('a')
test_links = test_nodes %>% html_attr('href')
test_text = test_nodes %>% html_text()
test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>%
filter(str_detect(test_links, 'glossary/.*/?char=')) %>%
select(test_text)
#  })    # t ~ 5 secs. It works.
# invoke func for each letter separately. More general that way than hard-coding A-Z also inside.
build_fair_dict <- function(test_url){
test_nodes <- test_url %>% read_html() %>% html_nodes('a')
test_links = test_nodes %>% html_attr('href')
test_text = test_nodes %>% html_text()
test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>%
filter(str_detect(test_links, 'glossary/.*/?char=')) %>%
select(test_text)
return(test_df)  } # func ends
## Looping over all the other letters
dict_content = vector(mode="list", length=length(LETTERS))    # list to store output
# system.time({
# commenting below coz of timeout issues I faced. Uncomment and run.
# for (i1 in 1:length(LETTERS)){     # type LETTERS in console and see
#   test_url = paste0(prefix_url, LETTERS[i1], suffix_url)
#   dict_content[[i1]] =  data.frame(LETTERS[i1], build_fair_dict(test_url))
#   cat(LETTERS[i1], " processed.", "\n")
#   }    # i1 loop ends
# })  # t = 127.13 secs
# Now bind list outp into df, check and close.
# suppressWarnings(dict_df = bind_rows(dict_content))
# str(dict_df)    # all good. :)
# Building a telecomindustry wordlist by scraping an alphabetical dict @ http://www.atis.org/glossary/
## Step 1: Load required libraries
library(rvest)
library(stringr)
library(dplyr)
library(magrittr)
# define tgt url
url <- 'https://glossary.atis.org/search-results/?char=A'
# extract url content with read_html() func
url_content <- read_html(url)
# alternately, above in 1 line.
# url_content <- read_html('http://www.atis.org/glossary/definitionsList.aspx?find=A&kw=0')
# alternately, using pipes operator %>%
url_content <- url %>% read_html()
# system.time({
# extracting all links in tgt page
links <-  url_content %>% html_nodes('a') %>%  # html_nodes() extracts ANY node type, as per argment
html_attr('href')    # html_attr() extracts particular info based on single node attribute
text <- url_content %>% html_nodes('a') %>% html_text()    # extracting ALL link text from tgt page
# }) # t = 0.84 secs
# creating a new dictionary of links and text extracted above
rough_dict <- data.frame(links, text, stringsAsFactors = F)
rough_dict[10:20,]
fair_dict <- rough_dict %>%
filter(str_detect(links, 'glossary/.*/?char=')) %>%
select(text)
tail(fair_dict)
## Testing above workflow for one letter
# first construct url.
prefix_url = 'https://glossary.atis.org/search-results/?char='
test_url = paste0(prefix_url, "B"); test_url    # OK. But does it work?
# system.time({
# run workflow for newly constructed url
test_nodes <- test_url %>% read_html() %>% html_nodes('a')
test_links = test_nodes %>% html_attr('href')
test_text = test_nodes %>% html_text()
test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>%
filter(str_detect(test_links, 'glossary/.*/?char=')) %>%
select(test_text)
#  })    # t ~ 5 secs. It works.
# invoke func for each letter separately. More general that way than hard-coding A-Z also inside.
build_fair_dict <- function(test_url){
test_nodes <- test_url %>% read_html() %>% html_nodes('a')
test_links = test_nodes %>% html_attr('href')
test_text = test_nodes %>% html_text()
test_df = data.frame(test_links, test_text, stringsAsFactors=FALSE) %>%
filter(str_detect(test_links, 'glossary/.*/?char=')) %>%
select(test_text)
return(test_df)  } # func ends
## Looping over all the other letters
dict_content = vector(mode="list", length=length(LETTERS))    # list to store output
# system.time({
# commenting below coz of timeout issues I faced. Uncomment and run.
# for (i1 in 1:length(LETTERS)){     # type LETTERS in console and see
#   test_url = paste0(prefix_url, LETTERS[i1], suffix_url)
#   dict_content[[i1]] =  data.frame(LETTERS[i1], build_fair_dict(test_url))
#   cat(LETTERS[i1], " processed.", "\n")
#   }    # i1 loop ends
# })  # t = 127.13 secs
# Now bind list outp into df, check and close.
# suppressWarnings(dict_df = bind_rows(dict_content))
# str(dict_df)    # all good. :)
library(rvest)
content <- read_html('https://news.ycombinator.com')
## key is to identify the right css selector or xpath values of the html elements
# system.time({
title <- content %>% html_nodes('a.storylink') %>% # use CSS selector tool to ID node 'a.storylink'
html_text()   # within above, retain only text.
head(title, 10)
## More CSS selections made and stored in R objects
score = content %>% html_nodes('span.score') %>% html_text()
age = content %>% html_nodes('span.age') %>% html_text()
## Stitch into a df now
df <- data.frame(title = title, score = score, age = age, stringsAsFactors=FALSE)
df %>% head(10)    # not working coz one link is missing when I tried it.
# just testing pulling out a full nodeset (tr.athing) instead of single node across nodesets.
athing_list = content %>% html_nodes('tr.athing')   # discovered 'athing' from page source as node-set element
length(athing_list)
# building empty vecs for colms
title_text = vector(mode="character", length=length(athing_list))
link_url = title_text    # so it has same list attribs as title_text.
source_site = title_text
# looping over each node-set
for (i1 in 1:length(athing_list)){
# some below may fail as nulls or NAs. Introducing html_attr() to get non-text content classes
a0 = athing_list[i1] %>% html_nodes('a.storylink') %>% html_text()
a1 = athing_list[i1] %>% html_nodes('a.storylink') %>% html_attr('href')
a2 = athing_list[i1] %>% html_nodes('span.sitestr') %>% html_text()
# seems empty vec outp was commonest error, so testing for the same using length()
ifelse(length(a0>0), {title_text[i1] = a0}, {title_text[i1] = "no title available"})
ifelse(length(a1>0), {link_url[i1] = a1}, {link_url[i1] = "no link available"})
ifelse(length(a2>0), {source_site[i1] = a2}, {source_site[i1] = "no site available"})
cat(i1, " processed.", "\n")
}    # i1 loop ends.  t = 0.78 secs
# now check if all lengths are same and then bind into a df
df = suppressWarnings(
data.frame(title_text, link_url, source_site, stringsAsFactors = FALSE))
head(df, 10)
View(df)
pnorm(-5/40)
pnorm(-5/(40/sqrt(100)))
pnorm(5/(40/sqrt(100)))
pnorm(-5/(40/sqrt(100))) + pnorm(5/(40/sqrt(100)))
pnorm(-5/(40/sqrt(100))) + (1 - pnorm(5/(40/sqrt(100))))
pnorm(.1/2)
5/sqrt(30)
5/sqrt(30)
library(MASS)
basicreg <- function(X,y) {
regdata <- lm(y ~ X)
coef <- summary(regdata)#$coefficients[2,1]
return(coef)
}
historical.data <- read.csv("Regression.csv")
print("Reliance Beta")
print(basicreg(historical.data$NSE.Return, historical.data$Reliance.Return))
print("HDFC Beta")
print(basicreg(historical.data$NSE.Return, historical.data$HDFC.Return))
setwd("~/Dropbox/isb/Business Fundamentals/Assignment")
library(MASS)
basicreg <- function(X,y) {
regdata <- lm(y ~ X)
coef <- summary(regdata)#$coefficients[2,1]
return(coef)
}
historical.data <- read.csv("Regression.csv")
print("Reliance Beta")
print(basicreg(historical.data$NSE.Return, historical.data$Reliance.Return))
print("HDFC Beta")
print(basicreg(historical.data$NSE.Return, historical.data$HDFC.Return))

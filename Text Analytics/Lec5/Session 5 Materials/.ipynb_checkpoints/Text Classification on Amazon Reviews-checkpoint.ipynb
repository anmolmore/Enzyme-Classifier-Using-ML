{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class,\n",
    "\n",
    "We now embark on a simple text-sentiment classification exercise using a host of methods/algorithms such as Naiva Bayes, SVM, Logistic regression, Random forests and boosted regressions the details of which you'll cover in later courses.\n",
    "\n",
    "Aim of this exercise is to demo the use of std libraries to do supervised classification exercises on labeled text data.\n",
    "\n",
    "See below\n",
    "\n",
    "## Importing the required libraries\n",
    "\n",
    "We'll use select components from the comprehensive *sklearn* library. Some select components of interest are:\n",
    "\n",
    "**model_selection** : for splitting the data into train and test dataset \n",
    "\n",
    "**preprocessing**: for encoding the label so that it can be used in the machine learning model\n",
    "\n",
    "**linear_model**: to implement linear models like linear regresson, logistic regression etc\n",
    "\n",
    "**naive_bayes**: for the class of Naive Bayes models\n",
    "\n",
    "**metrics**: For calculating the model metrics such as accuracy.\n",
    "\n",
    "**svm**: For implementing SVM or *Support Vector Machine* models. \n",
    "\n",
    "**TfidfVectorizer**: for calculating DTM under TD-IDF conditions\n",
    "\n",
    "**CountVectorizer**: for implementing DTM under TF\n",
    "\n",
    "**ensemble**: To implement Ensemble models like *Random Forests* \n",
    "\n",
    "**pandas**: For Dataframes\n",
    "\n",
    "If you're using the Anaconda py distribution, much of sklearn comes prepackaged. Else, might take some time to import. \n",
    "\n",
    "E.g., to import *xgboost*, type on command line: *$ pip install xgboost*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup chunk\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string  # $ pip install textblob\n",
    "import csv,re,nltk\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I loved the Da Vinci Code, but now I want some...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i thought da vinci code was great, same with k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0            The Da Vinci Code book is just awesome.     1\n",
       "1  this was the first clive cussler i've ever rea...     1\n",
       "2                   i liked the Da Vinci Code a lot.     1\n",
       "3                   i liked the Da Vinci Code a lot.     1\n",
       "4  I liked the Da Vinci Code but it ultimatly did...     1\n",
       "5  that's not even an exaggeration ) and at midni...     1\n",
       "6  I loved the Da Vinci Code, but now I want some...     1\n",
       "7  i thought da vinci code was great, same with k...     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data in\n",
    "filename = open(\"training.txt\",\"r\",encoding=\"utf-8\")\n",
    "sample_data = csv.reader(filename,delimiter = \"\\t\")\n",
    "labels, texts = [], []\n",
    "for i in sample_data:\n",
    "    labels.append(i[0])\n",
    "    texts.append(i[1])\n",
    "\n",
    "# build panda DF to house the data\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.iloc[:8,]  # view a few records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train & Test to Assess Model Accuracy\n",
    "\n",
    "Part of std operating proc in training machines on labeled data is (randomly) splitting the data into two parts - \n",
    "\n",
    "1] a *Training* or Calibration dataset on which the machine searches for the best function connecting labeled outcomes to data features\n",
    "\n",
    "2] a *Test* or Validation dataset on which the trained model is run to assess the accuracy of the model's predicted outcomes against known outcomes.\n",
    "\n",
    "We'll use sklearn's *model_selection.train_test_split()* func for the above. \n",
    "\n",
    "Then, in preprocessing we use a label *encoder* to setup the LHS, via func *preprocessing.LabelEncoder().fit_transform()*.\n",
    "\n",
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the DF into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable into 0/1\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DTM of the dataset \n",
    "\n",
    "Recall we'd built DTMs in R's tidytext with the cast_dtm() func. Now we see sklearn's approach to doing the same using the *CountVectorizer()* func. \n",
    "\n",
    "P.S. You can read more about the func's arguments and attributes [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15179705619812012 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1730, 2162)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a count vectorizer TF-DTM object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "t1 = time.time()\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2 - t1, \"secs\")  # ~ 0.15 secs to create DTM. Not bad, eh?\n",
    "xtrain_count.shape\n",
    "xvalid_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TFIDF on the word level for the dataset \n",
    "\n",
    "Recall TFIDF wale DTMs from the previous classes? We'd said that its hard to say a priori which might be more relevant or better-performing in any situ and that we should try both and see? \n",
    "\n",
    "Now, in this text classifn exercise, would it make more sense to use TF or an IDF wala DTM? Why not try both and see?\n",
    "\n",
    "Below, we use sklearn's *TfidfVectorizer()* func to build an IDF wala DTM.\n",
    "\n",
    "Behold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1310579776763916 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5188, 2162)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "t1 = time.time()\n",
    "tfidf_vect = tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2 - t1, \"secs\")  # ~ 0.15 secs again\n",
    "xtrain_tfidf.shape  # but many more tokens thsi time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model \n",
    "\n",
    "The arguments of the user defined func *train_model()* below allow you to:\n",
    "\n",
    "1] pick a classifier algorithm or method to run (P.S. see [here](https://scikit-learn.org/stable/supervised_learning.html) for a list)\n",
    "\n",
    "2] input the feature set and the labels\n",
    "\n",
    "3] both for training and validation datasets.\n",
    "\n",
    "Am setting *is_neural_net=False* as that is worth a whole other course some other time.\n",
    "\n",
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    #print(len(predictions))\n",
    "    predDF = pandas.DataFrame()\n",
    "    predDF['text'] = valid_x\n",
    "    predDF['actual_label'] = valid_y\n",
    "    predDF['model_label'] = predictions\n",
    "    \n",
    "    print(predDF.iloc[:8,])\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model using DTM and TFIDF\n",
    "\n",
    "Now rubber meets road. Let's use the *train_model* func above starting simple with Naive Bayes. \n",
    "\n",
    "Note how we output both actual and predicted label for each case to facilitate comparison.\n",
    "\n",
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "Naive Bayes on DTM accuracy: 0.9791907514450867\n",
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "Naive Bayes on WordLevel TF-IDF accuracy: 0.9722543352601156\n",
      "0.00859522819519043 secs for NB on TF\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on DTM\n",
    "t1 = time.time()\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "t2 = time.time()\n",
    "print(\"\\nNaive Bayes on DTM accuracy: \"+ str(accuracy))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"\\nNaive Bayes on WordLevel TF-IDF accuracy: \"+ str(accuracy))\n",
    "\n",
    "print(t2-t1, \"secs for NB on TF\")  # 0.01 secs. Fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier using DTM and TFIDF\n",
    "\n",
    "Same as above, we try logistic regression as our classification method this time.\n",
    "\n",
    "P.S. I'll expect more time here than in Naive Bayes which asumes conditional independence across cases.\n",
    "\n",
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "Logistic Regression on DTM Accuracy: 0.992485549132948\n",
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "Logistic Regression on WordLevel TF-IDF: 0.9901734104046243\n",
      "0.01581597328186035 secs for Logistic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmol/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on DTM\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"\\nLogistic Regression on DTM Accuracy: \"+ str(accuracy))\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "t1 = time.time()\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "t2 = time.time()\n",
    "print(\"\\nLogistic Regression on WordLevel TF-IDF: \"+ str(accuracy))\n",
    "\n",
    "print(t2-t1, \"secs for Logistic\")  # 0.02 secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM using DTM and TFIDF\n",
    "\n",
    "SVM is a more complex method than the previous two and I'll assume more time here for that reason. \n",
    "\n",
    "SVM tries to find a line or plane or hyperplane in high-dimensions (the so-called support-vector) that best seperates one class of labels from another. Again, details in later courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmol/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "SVM on DTM Model Accuracy: 0.9138728323699422\n",
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            1  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            1  \n",
      "5958            1  \n",
      "2549            1  \n",
      "\n",
      "SVM on TFIDF Accuracy: 0.5485549132947977\n",
      "1.736518144607544 secs in SVM\n"
     ]
    }
   ],
   "source": [
    "#SVM on DTM\n",
    "t1 = time.time()\n",
    "accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "t2 = time.time()\n",
    "print(\"\\nSVM on DTM Model Accuracy: \"+str(accuracy))\n",
    "\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"\\nSVM on TFIDF Accuracy: \"+str(accuracy))\n",
    "\n",
    "print(t2-t1, \"secs in SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha. Above we note that DTMs under TF and TFIDF weighing schemes actually produced dramatically different accuracy results under SVM.\n",
    "\n",
    "The simpler, smaller feature set under TF performed way better. \n",
    "\n",
    "## Random Forest using DTM and TFIDF\n",
    "\n",
    "Now we use a so-called *ensemble* method, i.e. a algorithm that runs a collection of classification tasks and averages or combines them to yield a final output. Often considered advantageous over one-shot methods in complex tasks.\n",
    "\n",
    "A random forest, as the name implies, is an ensemble of decision trees. The random forest is actually a forest of decision trees grown from different starting points using different feature subsets and later combined to  gove an overall picture.\n",
    "\n",
    "See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "Random Forest on DTM Accuracy:  0.9838150289017341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anmol/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/anmol/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "Random Forest on WordLevel TF-IDF Accuracy:  0.9809248554913295\n",
      "0.12029004096984863 secs under RF\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"\\nRandom Forest on DTM Accuracy: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "t1 = time.time()\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "t2 = time.time()\n",
    "print(\"\\nRandom Forest on WordLevel TF-IDF Accuracy: \", accuracy)\n",
    "\n",
    "print(t2-t1, \"secs under RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting using DTM and TFIDF\n",
    "\n",
    "Finally, our last classifn method for the day - gradient boosting that follows 'boosting' principles. IOW, it focusses on misclassified cases and assigns more weight to learning from misclassified cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "XGBoost on DTM Accuracy:  0.9815028901734104\n",
      "                                                   text  actual_label  \\\n",
      "1323  So as felicia's mom is cleaning the table, fel...             1   \n",
      "4874                             da vinci code sucks...             0   \n",
      "2683  I love the Harry Potter series if you can coun...             1   \n",
      "3169              dudeee i LOVED brokeback mountain!!!!             1   \n",
      "3100                      I LOVE Brokeback Mountain!!!.             1   \n",
      "4042                           the Da Vinci Code sucks.             0   \n",
      "5958                         brokeback mountain sucks..             0   \n",
      "2549                               I love Harry Potter.             1   \n",
      "\n",
      "      model_label  \n",
      "1323            1  \n",
      "4874            0  \n",
      "2683            1  \n",
      "3169            1  \n",
      "3100            1  \n",
      "4042            0  \n",
      "5958            0  \n",
      "2549            1  \n",
      "\n",
      "XGBoost on DTM Accuracy WordLevel TF-IDF:  0.9809248554913295\n",
      "0.7416820526123047 secs under xgboost\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "t1 = time.time()\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "t2 = time.time()\n",
    "print(\"\\nXGBoost on DTM Accuracy: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"\\nXGBoost on DTM Accuracy WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "print(t2-t1, \"secs under xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, so far so good. What Qs come to your mind?\n",
    "\n",
    "What do you think? Which method performed *best*? Which DTM weighing scheme seems superior for this application? \n",
    "\n",
    "Can we use these methods if the number of classes exceeds 2? Etc.\n",
    "\n",
    "Sudhir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

---
title: "Zomato Reviews"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Import necessary libraries

```{r}
suppressPackageStartupMessages({
  library(rvest) #for webscrapping
  library(dplyr) #for dataframe operations
  library(RSelenium) #for selenium web driver
})
```

#reviewer user id - each user is unique on zomato and using this, we can identify how old/new user is on zomato. Same can be used to navigate to user profile (even if users change their name)
#review text - can be used for text analytics
#review rating - important to scrap from analytics perspective
#review time - time of review

```{r message=FALSE}
ReadReviewsForResturant <- function(html_page) {
    reviews.df <- data.frame(matrix(ncol = 7, nrow = 0))
    headers <- c("UserId", "Review Text", "Rating", "Time", "Reviews Count", "Followers Count", "Photos Count")
    colnames(reviews.df) <- headers
  
    reviewer.id <- html_page %>% html_nodes('.res-large-snippet .header a') %>% html_attr('data-entity_id')
    review.text <- substring(gsub("\n\\s+", "", html_page %>% html_nodes('.rev-text') %>% html_text()), 7)
    review.rating <- as.numeric(substring((html_page %>% 
                                             html_nodes('.rev-text div') %>% 
                                             html_attr('aria-label'))[c(TRUE, FALSE)], 7))
    review.time <- html_page %>% html_nodes('time') %>% html_attr('datetime')
    
    current.chunk <- data.frame(reviewer.id, review.text, review.rating, review.time, 0, 0, 0)
    names(current.chunk) <- headers
    reviews.df <- rbind(reviews.df, current.chunk)
    return(reviews.df)
}

```

# Iterate through each users page, and collect metadata related to reviews count, followers count, photos count
# Append values to reviews dataframe

```{r}
GetUsersMetadata <- function(reviews.df) {
  for (id in reviews.df$UserId) {
    user.url <- paste0("https://www.zomato.com/users/",id)
    print(user.url)
    html_page <- user.url %>% read_html()
    
    user.metadata.total.reviews.count <- html_page %>% html_node('.user-tab-reviews .ui.label') %>% html_text()
    user.metadata.followers.count <- html_page %>% html_node('.user-tab-follows .ui.label') %>% html_text()
    user.metadata.photos.count <- html_page %>% html_node('.user-tab-photos .ui.label') %>% html_text()
  
    reviews.df[reviews.df$UserId == id, "Reviews Count"] <- user.metadata.total.reviews.count
    reviews.df[reviews.df$UserId == id, "Followers Count"] <- user.metadata.followers.count
    reviews.df[reviews.df$UserId == id, "Photos Count"] <- user.metadata.photos.count
    
    Sys.sleep(10)
  }
  return(reviews.df)
}
```

#Click on load more n times, to load all reviews from page

```{r message=FALSE}
LoadAllRestaurantReviews <- function(url) {
  driver <- rsDriver(browser=c("chrome"), port=4444L)
  browser <- driver[["client"]]

  browser$open()  #open client and navigate to restaurant reviews url
  browser$navigate(url)

  Sys.sleep(5)
  
  all.reviews.link <- tryCatch({
    browser$findElement(using = 'xpath', "//*[contains(text(), 'All Reviews')]")
  }, error = function(NoSuchElementException) {
    browser$close()
    driver[["server"]]$stop()
    message("No reviews available, skip !")
  })
  if(!is.null(all.reviews.link)) {
    all.reviews.link$clickElement()
    
    #Page takes a while to to load
    Sys.sleep(3)
  
    #Click on search button and print title
    for(i in 1:100) {
      tryCatch({
        load.more.button <- browser$findElement(using = 'class', "zs-load-more-count")
        if(!is.null(load.more.button)) {
          load.more.button$clickElement()
          Sys.sleep(3)
        }
        else {
          #All reviews visible - No more load-more elements present
          break
        }
      }, error = function(NoSuchElementException) {
        message("All reviews loaded, continue !")
        break
      }) 
    }
    
    page_source <- browser$getPageSource() # read in page contents
    html_page <- read_html(page_source[[1]])
    browser$close()
    
    # stop the selenium server
    driver[["server"]]$stop()
    
    return(html_page)
  }
}
```

# iterate through each restaurant and fetch reviews and users metadata and write to csv 

```{r message=FALSE}
#start with base url of 1 restaurant
#Later iterate through all restaurant

system.time({
  restaurant.list <- read.csv("restaurant_list.csv")
  for(row in 1:2) { #nrow(restaurant.list)
    url <- paste0(restaurant.list[row, "url"],"/reviews")
    restaurant.name <- restaurant.list[row, "restaurant_name"]
    print(paste("Fetching reviews for :", url))
    restaurant.page <- LoadAllRestaurantReviews(url)
    if(!is.null(restaurant.page)) {
      system.time({
        system.time({
          reviews.df <- ReadReviewsForResturant(restaurant.page)
        })
        system.time({
          reviews.df <- GetUsersMetadata(reviews.df)
        })
        write.csv(reviews.df, paste0(restaurant.name, ".csv"))
      })
    }
  }
})
```

#Write collected restaurant list to csv file to avoid losing

```{r}
reviews.df <- read.csv("Flechazo.csv")
print(reviews.df$UserId)
```

